# Optimisers.jl

<!-- [![][docs-stable-img]][docs-stable-url] -->
[![][docs-dev-img]][docs-dev-url]
[![][action-img]][action-url]
[![][coverage-img]][coverage-url] 

[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg
[docs-stable-url]: https://fluxml.ai/Optimisers.jl/stable/

[docs-dev-img]: https://img.shields.io/badge/docs-dev-blue.svg
[docs-dev-url]: https://fluxml.ai/Optimisers.jl/dev/

[action-img]: https://github.com/FluxML/Optimisers.jl/workflows/CI/badge.svg
[action-url]: https://github.com/FluxML/Optimisers.jl/actions

[coverage-img]: https://codecov.io/gh/FluxML/Optimisers.jl/branch/master/graph/badge.svg
[coverage-url]: https://codecov.io/gh/FluxML/Optimisers.jl

Optimisers.jl defines many standard gradient-based optimisation rules, and tools for applying them to deeply nested models.

This is the future of training for [Flux.jl](https://github.com/FluxML/Flux.jl) neural networks,
but it can be used separately on anything understood by [Functors.jl](https://github.com/FluxML/Functors.jl).

## Installation

```julia
]add Optimisers
```

## Usage

Find out more about using Optimisers.jl [in the docs](https://fluxml.ai/Optimisers.jl/dev/).
