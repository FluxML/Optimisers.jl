<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · Optimisers.jl</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://fluxml.ai/Optimisers.jl/stable/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="Optimisers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Optimisers.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#Optimisation-Rules"><span>Optimisation Rules</span></a></li><li><a class="tocitem" href="#Model-Interface"><span>Model Interface</span></a></li><li><a class="tocitem" href="#Rule-Definition"><span>Rule Definition</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Optimisers.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h2 id="Optimisation-Rules"><a class="docs-heading-anchor" href="#Optimisation-Rules">Optimisation Rules</a><a id="Optimisation-Rules-1"></a><a class="docs-heading-anchor-permalink" href="#Optimisation-Rules" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Descent" href="#Optimisers.Descent"><code>Optimisers.Descent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Descent(η = 1f-1)</code></pre><p>Classic gradient descent optimiser with learning rate <code>η</code>. For each parameter <code>p</code> and its gradient <code>dp</code>, this runs <code>p -= η*dp</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L9-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Momentum" href="#Optimisers.Momentum"><code>Optimisers.Momentum</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Momentum(η = 1f-2, ρ = 9f-1)</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L32-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Nesterov" href="#Optimisers.Nesterov"><code>Optimisers.Nesterov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Nesterov(η = 1f-3, ρ = 9f-1)</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and Nesterov momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Nesterov momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                          prominent direction, in effect dampening oscillations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L58-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Rprop" href="#Optimisers.Rprop"><code>Optimisers.Rprop</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Rprop(η = 1f-3, ℓ = (5f-1, 1.2f0), Γ = (1f-6, 50f0))</code></pre><p>Optimizer using the <a href="https://ieeexplore.ieee.org/document/298623">Rprop</a> algorithm. A full-batch learning algorithm that depends only on the sign of the gradient.</p><p><strong>Parameters</strong></p><ul><li><p>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</p></li><li><p>Scaling factors (<code>ℓ::Tuple</code>): Multiplicative increase and decrease factors.</p></li><li><p>Step sizes (<code>Γ::Tuple</code>): Mminimal and maximal allowed step sizes.</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L145-L159">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.RMSProp" href="#Optimisers.RMSProp"><code>Optimisers.RMSProp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RMSProp(η = 1f-3, ρ = 9f-1, ϵ = eps(typeof(η)); centred = false)</code></pre><p>Optimizer using the <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a> algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don&#39;t need tuning.</p><p><a href="http://arxiv.org/abs/1308.08500">Centred RMSProp</a> is a variant which normalises gradients by an estimate their variance, instead of their second moment.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li><li>Keyword <code>centred</code> (or <code>centered</code>): Indicates whether to use centred variant                                    of the algorithm.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L86-L106">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Adam" href="#Optimisers.Adam"><code>Optimisers.Adam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Adam(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1412.6980">Adam</a> optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L187-L199">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.RAdam" href="#Optimisers.RAdam"><code>Optimisers.RAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RAdam(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1908.03265">Rectified Adam</a> optimizer.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L220-L232">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.AdaMax" href="#Optimisers.AdaMax"><code>Optimisers.AdaMax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaMax(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1412.6980">AdaMax</a> is a variant of Adam based on the ∞-norm.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L261-L273">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.OAdam" href="#Optimisers.OAdam"><code>Optimisers.OAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OAdam(η = 1f-3, β = (5f-1, 9f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1711.00141">OAdam</a> (Optimistic Adam) is a variant of Adam adding an &quot;optimistic&quot; term suitable for adversarial training.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L294-L307">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.AdaGrad" href="#Optimisers.AdaGrad"><code>Optimisers.AdaGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaGrad(η = 1f-1, ϵ = eps(typeof(η)))</code></pre><p><a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">AdaGrad</a> optimizer. It has parameter specific learning rates based on how frequently it is updated. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L330-L342">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.AdaDelta" href="#Optimisers.AdaDelta"><code>Optimisers.AdaDelta</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaDelta(ρ = 9f-1, ϵ = eps(typeof(ρ)))</code></pre><p><a href="https://arxiv.org/abs/1212.5701">AdaDelta</a> is a version of AdaGrad adapting its learning rate based on a window of past gradient updates. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Rho (<code>ρ</code>): Factor by which the gradient is decayed at each time step.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L361-L372">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.AMSGrad" href="#Optimisers.AMSGrad"><code>Optimisers.AMSGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AMSGrad(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p>The <a href="https://openreview.net/forum?id=ryQu7f-RZ">AMSGrad</a> version of the Adam optimiser. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L393-L406">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.NAdam" href="#Optimisers.NAdam"><code>Optimisers.NAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NAdam(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ">NAdam</a> is a Nesterov variant of Adam. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L429-L442">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.AdamW" href="#Optimisers.AdamW"><code>Optimisers.AdamW</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">AdamW(η = 1f-3, β = (9f-1, 9.99f-1), γ = 0, ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1711.05101">AdamW</a> is a variant of Adam fixing (as in repairing) its weight decay regularization.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Weight decay (<code>γ</code>): Decay applied to weights during optimisation.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L465-L479">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.AdaBelief" href="#Optimisers.AdaBelief"><code>Optimisers.AdaBelief</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaBelief(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = 1e-16)</code></pre><p>The <a href="https://arxiv.org/abs/2010.07468">AdaBelief</a> optimiser is a variant of the well-known Adam optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ::Float32</code>): Constant to prevent division by zero                                 (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L483-L496">source</a></section></article><p>In addition to the main course, you may wish to order some of these condiments:</p><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ClipGrad" href="#Optimisers.ClipGrad"><code>Optimisers.ClipGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClipGrad(δ = 10f0)</code></pre><p>Restricts every gradient component to obey <code>-δ ≤ dx[i] ≤ δ</code>.</p><p>See also <a href="#Optimisers.ClipNorm"><code>ClipNorm</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L542-L548">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ClipNorm" href="#Optimisers.ClipNorm"><code>Optimisers.ClipNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClipNorm(ω = 10f0, p = 2; throw = true)</code></pre><p>Scales any gradient array for which <code>norm(dx, p) &gt; ω</code> to stay at this threshold (unless <code>p==0</code>).</p><p>Throws an error if the norm is infinite or <code>NaN</code>, which you can turn off with <code>throw = false</code>.</p><p>See also <a href="#Optimisers.ClipGrad"><code>ClipGrad</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L563-L573">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.WeightDecay" href="#Optimisers.WeightDecay"><code>Optimisers.WeightDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">WeightDecay(γ = 5f-4)</code></pre><p>Decay weights by <span>$γ$</span>, that is, add <code>γ .* x</code> to the gradient <code>x̄</code> which will be subtracted from <code>x</code>.</p><p>Typically composed  with other optimisers as the first transformation in an <a href="#Optimisers.OptimiserChain"><code>OptimiserChain</code></a>. This is equivalent to adding <span>$L_2$</span> regularization with coefficient <span>$γ$</span> to the loss.</p><p><strong>Parameters</strong></p><ul><li>Weight decay (<code>γ</code>): Decay applied to weights during optimisation.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L517-L528">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.OptimiserChain" href="#Optimisers.OptimiserChain"><code>Optimisers.OptimiserChain</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OptimiserChain(opts...)</code></pre><p>Compose a sequence of optimisers so that each <code>opt</code> in <code>opts</code> updates the gradient, in the order specified.</p><p>With an empty sequence, <code>OptimiserChain()</code> is the identity, so <code>update!</code> will subtract the full gradient from the parameters. This is equivalent to <code>Descent(1)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; o = OptimiserChain(ClipGrad(1.0), Descent(0.1));

julia&gt; m = (zeros(3),);

julia&gt; s = Optimisers.setup(o, m)
(Leaf(OptimiserChain(ClipGrad{Float64}(1.0), Descent{Float64}(0.1)), (nothing, nothing)),)

julia&gt; Optimisers.update(s, m, ([0.3, 1, 7],))[2]  # clips before discounting
([-0.03, -0.1, -0.1],)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/rules.jl#L593-L615">source</a></section></article><h2 id="Model-Interface"><a class="docs-heading-anchor" href="#Model-Interface">Model Interface</a><a id="Model-Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Interface" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Optimisers.setup" href="#Optimisers.setup"><code>Optimisers.setup</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.setup(rule, model) -&gt; tree</code></pre><p>Initialises the given optimiser for every trainable parameter within the model. Returns a tree of the relevant states, which must be passed to <a href="#Optimisers.update"><code>update</code></a> or <a href="#Optimisers.update!"><code>update!</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x = rand(3), y = (true, false), z = tanh);

julia&gt; Optimisers.setup(Momentum(), m)  # same field names as m
(x = Leaf(Momentum{Float32}(0.01, 0.9), [0.0, 0.0, 0.0]), y = ((), ()), z = ())</code></pre><p>The recursion into structures uses Functors.jl, and any new <code>struct</code>s containing parameters need to be marked with <code>Functors.@functor</code> before use. See <a href="https://fluxml.ai/Flux.jl/stable/models/advanced/">the Flux docs</a> for more about this.</p><pre><code class="language-julia-repl hljs">julia&gt; struct Layer; mat; fun; end

julia&gt; model = (lay = Layer([1 2; 3 4f0], sin), vec = [5, 6f0]);

julia&gt; Optimisers.setup(Momentum(), model)  # new struct is by default ignored
(lay = (), vec = Leaf(Momentum{Float32}(0.01, 0.9), Float32[0.0, 0.0]))

julia&gt; destructure(model)
(Float32[5.0, 6.0], Restructure(NamedTuple, ..., 2))

julia&gt; using Functors; @functor Layer  # annotate this type as containing parameters

julia&gt; Optimisers.setup(Momentum(), model)
(lay = (mat = Leaf(Momentum{Float32}(0.01, 0.9), Float32[0.0 0.0; 0.0 0.0]), fun = ()), vec = Leaf(Momentum{Float32}(0.01, 0.9), Float32[0.0, 0.0]))

julia&gt; destructure(model)
(Float32[1.0, 3.0, 2.0, 4.0, 5.0, 6.0], Restructure(NamedTuple, ..., 6))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/Optimisers.jl#L68-L106">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.update" href="#Optimisers.update"><code>Optimisers.update</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.update(tree, model, gradient) -&gt; (tree, model)</code></pre><p>Uses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from <a href="#Optimisers.setup"><code>setup</code></a>.</p><p>See also <a href="#Optimisers.update!"><code>update!</code></a>, which will be faster for models of ordinary <code>Array</code>s or <code>CuArray</code>s.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x = Float32[1,2,3], y = tanh);

julia&gt; t = Optimisers.setup(Descent(0.1f0), m)
(x = Leaf(Descent{Float32}(0.1), nothing), y = ())

julia&gt; g = (x = [1,1,1], y = nothing);  # fake gradient

julia&gt; Optimisers.update(t, m, g)
((x = Leaf(Descent{Float32}(0.1), nothing), y = ()), (x = Float32[0.9, 1.9, 2.9], y = tanh))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/Optimisers.jl#L109-L130">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.update!" href="#Optimisers.update!"><code>Optimisers.update!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.update!(tree, model, gradient) -&gt; (tree, model)</code></pre><p>Uses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from <a href="#Optimisers.setup"><code>setup</code></a>.</p><p>This is used in exactly the same manner as <a href="#Optimisers.update"><code>update</code></a>, but because it may mutate arrays within the old model (and the old state), it will be faster for models of ordinary <code>Array</code>s or <code>CuArray</code>s. However, you should not rely on the old model being fully updated but rather use the returned model.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using StaticArrays, Zygote, Optimisers

julia&gt; m = (x = [1f0, 2f0], y = SA[4f0, 5f0]);  # partly mutable model

julia&gt; t = Optimisers.setup(Momentum(1/30, 0.9), m);

julia&gt; g = gradient(m -&gt; sum(abs2.(m.x .+ m.y)), m)[1]
(x = Float32[10.0, 14.0], y = Float32[10.0, 14.0])

julia&gt; t2, m2 = Optimisers.update!(t, m, g);

julia&gt; m2  # after update or update!, this is the new model
(x = Float32[0.6666666, 1.5333333], y = Float32[3.6666667, 4.5333333])

julia&gt; m2.x === m.x  # update! has re-used this array, for efficiency
true

julia&gt; m  # original should be discarded, may be mutated but no guarantee
(x = Float32[0.6666666, 1.5333333], y = Float32[4.0, 5.0])

julia&gt; t == t2  # original state is in fact guaranteed to be mutated
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/Optimisers.jl#L133-L171">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.adjust!" href="#Optimisers.adjust!"><code>Optimisers.adjust!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.adjust!(tree, η)</code></pre><p>Alters the state <code>tree = setup(rule, model)</code> to change the parameters of the optimisation rule, without destroying its stored state. Typically used mid-way through training.</p><p>Can be applied to part of a model, by acting only on the corresponding part of the state <code>tree</code>.</p><p>To change just the learning rate, provide a number <code>η::Real</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (vec = rand(Float32, 2), fun = sin);

julia&gt; st = Optimisers.setup(Nesterov(), m)  # stored momentum is initialised to zero
(vec = Leaf(Nesterov{Float32}(0.001, 0.9), Float32[0.0, 0.0]), fun = ())

julia&gt; st, m = Optimisers.update(st, m, (vec = [16, 88], fun = nothing));  # with fake gradient

julia&gt; st
(vec = Leaf(Nesterov{Float32}(0.001, 0.9), Float32[-0.016, -0.088]), fun = ())

julia&gt; Optimisers.adjust!(st, 0.123)  # change learning rate, stored momentum untouched

julia&gt; st
(vec = Leaf(Nesterov{Float32}(0.123, 0.9), Float32[-0.016, -0.088]), fun = ())</code></pre><p>To change other parameters, <code>adjust!</code> also accepts keyword arguments matching the field names of the optimisation rule&#39;s type.</p><pre><code class="language-julia-repl hljs">julia&gt; fieldnames(Adam)
(:eta, :beta, :epsilon)

julia&gt; st2 = Optimisers.setup(OptimiserChain(ClipGrad(), Adam()), m)
(vec = Leaf(OptimiserChain(ClipGrad{Float32}(10.0), Adam{Float32}(0.001, (0.9, 0.999), 1.19209f-7)), (nothing, (Float32[0.0, 0.0], Float32[0.0, 0.0], (0.9, 0.999)))), fun = ())

julia&gt; Optimisers.adjust(st2; beta = (0.777, 0.909), delta = 11.1)  # delta acts on ClipGrad
(vec = Leaf(OptimiserChain(ClipGrad{Float32}(11.1), Adam{Float32}(0.001, (0.777, 0.909), 1.19209f-7)), (nothing, (Float32[0.0, 0.0], Float32[0.0, 0.0], (0.9, 0.999)))), fun = ())

julia&gt; Optimisers.adjust(st; beta = &quot;no such field&quot;)  # silently ignored!
(vec = Leaf(Nesterov{Float32}(0.123, 0.9), Float32[-0.016, -0.088]), fun = ())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/adjust.jl#L58-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.adjust-Tuple{Any, Real}" href="#Optimisers.adjust-Tuple{Any, Real}"><code>Optimisers.adjust</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">adjust(tree, η) -&gt; tree</code></pre><p>Like <a href="#Optimisers.adjust-Tuple{Any, Real}"><code>adjust!</code></a>, but returns a new tree instead of mutating the old one.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/adjust.jl#L114-L118">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.freeze!" href="#Optimisers.freeze!"><code>Optimisers.freeze!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.freeze!(tree)</code></pre><p>Temporarily alters the state <code>tree = setup(rule, model)</code> so that parameters will not be updated. Un-done by <a href="#Optimisers.thaw!"><code>thaw!</code></a>.</p><p>Can be applied to the state corresponding to only part of a model, for instance with <code>model::Chain</code>, to freeze <code>model.layers[1]</code> you should call <code>freeze!(tree.layers[1])</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x = ([1.0], 2.0), y = [3.0]);

julia&gt; s = Optimisers.setup(Momentum(), m);

julia&gt; Optimisers.freeze!(s.x)

julia&gt; Optimisers.update!(s, m, (x = ([pi], 10pi), y = [100pi]));  # with fake gradient

julia&gt; m
(x = ([1.0], 2.0), y = [-0.14159258336972558])

julia&gt; s
(x = (Leaf(Momentum{Float32}(0.01, 0.9), [0.0], frozen = true), ()), y = Leaf(Momentum{Float32}(0.01, 0.9), [3.14159]))

julia&gt; Optimisers.thaw!(s)

julia&gt; s.x
(Leaf(Momentum{Float32}(0.01, 0.9), [0.0]), ())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/adjust.jl#L5-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.thaw!" href="#Optimisers.thaw!"><code>Optimisers.thaw!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.thaw!(tree)</code></pre><p>The reverse of <a href="#Optimisers.freeze!"><code>freeze!</code></a>. Applies to all parameters, mutating every <code>Leaf(rule, state, frozen = true)</code> to <code>Leaf(rule, state, frozen = false)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/adjust.jl#L40-L45">source</a></section></article><p>Calling <code>Functors.@functor</code> on your model&#39;s layer types by default causes these functions to recurse into all children, and ultimately optimise all <code>isnumeric</code> leaf nodes. To further restrict this by ignoring some fields of a layer type, define <code>trainable</code>:</p><article class="docstring"><header><a class="docstring-binding" id="Optimisers.trainable" href="#Optimisers.trainable"><code>Optimisers.trainable</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trainable(x::Layer) -&gt; NamedTuple</code></pre><p>This should be overloaded to make optimisers ignore some fields of every <code>Layer</code>, which would otherwise contain trainable parameters. (Elements such as functions and sizes are always ignored.)</p><p>The default is <code>Functors.children(x)</code>, usually a NamedTuple of all fields, and <code>trainable(x)</code> must contain a subset of these.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/interface.jl#L147-L156">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.isnumeric" href="#Optimisers.isnumeric"><code>Optimisers.isnumeric</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">isnumeric(x) -&gt; Bool</code></pre><p>Returns <code>true</code> on any parameter to be adjusted by Optimisers.jl, namely arrays of non-integer numbers. Returns <code>false</code> on all other types.</p><p>Requires also that <code>Functors.isleaf(x) == true</code>, to focus on e.g. the parent of a transposed matrix, not the wrapper.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/interface.jl#L122-L130">source</a></section></article><p>Such restrictions are also obeyed by this function for flattening a model:</p><article class="docstring"><header><a class="docstring-binding" id="Optimisers.destructure" href="#Optimisers.destructure"><code>Optimisers.destructure</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">destructure(model) -&gt; vector, reconstructor</code></pre><p>Copies all <a href="#Optimisers.trainable"><code>trainable</code></a>, <a href="#Optimisers.isnumeric"><code>isnumeric</code></a> parameters in the model to a vector, and returns also a function which reverses this transformation. Differentiable.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; v, re = destructure((x=[1.0, 2.0], y=(sin, [3.0 + 4.0im])))
(ComplexF64[1.0 + 0.0im, 2.0 + 0.0im, 3.0 + 4.0im], Restructure(NamedTuple, ..., 3))

julia&gt; re([3, 5, 7+11im])
(x = [3.0, 5.0], y = (sin, ComplexF64[7.0 + 11.0im]))</code></pre><p>If <code>model</code> contains various number types, they are promoted to make <code>vector</code>, and are usually restored by <code>Restructure</code>. Such restoration follows the rules  of <code>ChainRulesCore.ProjectTo</code>, and thus will restore floating point precision, but will permit more exotic numbers like <code>ForwardDiff.Dual</code>.</p><p>If <code>model</code> contains only GPU arrays, then <code>vector</code> will also live on the GPU. At present, a mixture of GPU and ordinary CPU arrays is undefined behaviour.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/destructure.jl#L5-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Restructure" href="#Optimisers.Restructure"><code>Optimisers.Restructure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Restructure(Model, ..., length)</code></pre><p>This is what <a href="#Optimisers.destructure"><code>destructure</code></a> returns, and <code>re(p)</code> will re-build the model with new parameters from vector <code>p</code>. If the model is callable, then <code>re(x, p) == re(p)(x)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; using Flux, Optimisers

julia&gt; _, re = destructure(Dense([1 2; 3 4], [0, 0], sigmoid))
([1, 3, 2, 4, 0, 0], Restructure(Dense, ..., 6))

julia&gt; m = re(-4:1)
Dense(2, 2, σ)      # 6 parameters

julia&gt; m([0.2, 0.3]) ≈ re([0.2, 0.3], -4:1)
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/destructure.jl#L34-L53">source</a></section></article><h2 id="Rule-Definition"><a class="docs-heading-anchor" href="#Rule-Definition">Rule Definition</a><a id="Rule-Definition-1"></a><a class="docs-heading-anchor-permalink" href="#Rule-Definition" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Optimisers.apply!" href="#Optimisers.apply!"><code>Optimisers.apply!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.apply!(rule::RuleType, state, parameters, gradient) -&gt; (state, gradient)</code></pre><p>This defines the action of any optimisation rule. It should return the modified gradient which will be subtracted from the parameters, and the updated state (if any) for use at the next iteration, as a tuple <code>(state, gradient)</code>.</p><p>For efficiency it is free to mutate the old state, but only what is returned will be used. Ideally this should check <code>maywrite(x)</code>, which the built-in rules do via <a href="#Optimisers.@.."><code>@..</code></a>.</p><p>The initial state is <code>init(rule::RuleType, parameters)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Optimisers.init(Descent(0.1), Float32[1,2,3]) === nothing
true

julia&gt; Optimisers.apply!(Descent(0.1), nothing, Float32[1,2,3], [4,5,6])
(nothing, Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}}(*, ([4, 5, 6], 0.1f0)))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/Optimisers.jl#L23-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.init" href="#Optimisers.init"><code>Optimisers.init</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.init(rule::RuleType, parameters) -&gt; state</code></pre><p>Sets up the initial state for a given optimisation rule, and an array of parameters. This and <a href="#Optimisers.apply!"><code>apply!</code></a> are the two functions which any new optimisation rule must define.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Optimisers.init(Descent(), Float32[1,2,3])  # is `nothing`

julia&gt; Optimisers.init(Momentum(), [1.0, 2.0])
2-element Vector{Float64}:
 0.0
 0.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/Optimisers.jl#L46-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.@.." href="#Optimisers.@.."><code>Optimisers.@..</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">@.. x = y + z</code></pre><p>Sometimes in-place broadcasting macro, for use in <code>apply!</code> rules. If <code>maywrite(x)</code> then it is just <code>@. x = rhs</code>, but if not, it becomes <code>x = @. rhs</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/interface.jl#L172-L177">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.@lazy" href="#Optimisers.@lazy"><code>Optimisers.@lazy</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">x = @lazy y + z</code></pre><p>Lazy broadcasting macro, for use in <code>apply!</code> rules. It broadcasts like <code>@.</code> but does not materialise, returning a <code>Broadcasted</code> object for later use. Beware that mutation of arguments will affect the result, and that if it is used in two places, work will be done twice.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/interface.jl#L189-L196">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.adjust-Tuple{AbstractRule, Real}" href="#Optimisers.adjust-Tuple{AbstractRule, Real}"><code>Optimisers.adjust</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">Optimisers.adjust(rule::RuleType, η::Real) -&gt; rule</code></pre><p>If a new optimisation rule has a learning rate which is <em>not</em> stored in field <code>rule.eta</code>, then you may should add a method to <code>adjust</code>. (But simpler to just use the standard name.)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/0b2d32b8973268dc510cf39784fcbe140d07cd7a/src/adjust.jl#L130-L135">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Sunday 27 November 2022 04:42">Sunday 27 November 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
