<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · Optimisers.jl</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://fluxml.ai/Optimisers.jl/stable/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Optimisers.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#Optimisation-Rules"><span>Optimisation Rules</span></a></li><li><a class="tocitem" href="#Model-Interface"><span>Model Interface</span></a></li><li><a class="tocitem" href="#Rule-Definition"><span>Rule Definition</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Optimisers.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h2 id="Optimisation-Rules"><a class="docs-heading-anchor" href="#Optimisation-Rules">Optimisation Rules</a><a id="Optimisation-Rules-1"></a><a class="docs-heading-anchor-permalink" href="#Optimisation-Rules" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Descent" href="#Optimisers.Descent"><code>Optimisers.Descent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Descent(η = 1f-1)</code></pre><p>Classic gradient descent optimiser with learning rate <code>η</code>. For each parameter <code>p</code> and its gradient <code>dp</code>, this runs <code>p -= η*dp</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L1-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Momentum" href="#Optimisers.Momentum"><code>Optimisers.Momentum</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Momentum(η = 1f-2, ρ = 9f-1)</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L24-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Nesterov" href="#Optimisers.Nesterov"><code>Optimisers.Nesterov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Nesterov(η = 1f-3, ρ = 9f-1)</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and Nesterov momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Nesterov momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                          prominent direction, in effect dampening oscillations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L50-L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.RMSProp" href="#Optimisers.RMSProp"><code>Optimisers.RMSProp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RMSProp(η = 1f-3, ρ = 9f-1, ϵ = eps(typeof(η)))</code></pre><p>Optimizer using the <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a> algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L78-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ADAM" href="#Optimisers.ADAM"><code>Optimisers.ADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ADAM(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1412.6980">ADAM</a> optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L112-L124">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.RADAM" href="#Optimisers.RADAM"><code>Optimisers.RADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RADAM(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1908.03265">Rectified ADAM</a> optimizer.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L145-L157">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.AdaMax" href="#Optimisers.AdaMax"><code>Optimisers.AdaMax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaMax(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1412.6980">AdaMax</a> is a variant of ADAM based on the ∞-norm.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L186-L198">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.OADAM" href="#Optimisers.OADAM"><code>Optimisers.OADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OADAM(η = 1f-3, β = (5f-1, 9f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1711.00141">OADAM</a> (Optimistic ADAM) is a variant of ADAM adding an &quot;optimistic&quot; term suitable for adversarial training.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L219-L232">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ADAGrad" href="#Optimisers.ADAGrad"><code>Optimisers.ADAGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ADAGrad(η = 1f-1, ϵ = eps(typeof(η)))</code></pre><p><a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">ADAGrad</a> optimizer. It has parameter specific learning rates based on how frequently it is updated. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L255-L267">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ADADelta" href="#Optimisers.ADADelta"><code>Optimisers.ADADelta</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ADADelta(ρ = 9f-1, ϵ = eps(typeof(ρ)))</code></pre><p><a href="https://arxiv.org/abs/1212.5701">ADADelta</a> is a version of ADAGrad adapting its learning rate based on a window of past gradient updates. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Rho (<code>ρ</code>): Factor by which the gradient is decayed at each time step.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L286-L297">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.AMSGrad" href="#Optimisers.AMSGrad"><code>Optimisers.AMSGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AMSGrad(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p>The <a href="https://openreview.net/forum?id=ryQu7f-RZ">AMSGrad</a> version of the ADAM optimiser. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L318-L331">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.NADAM" href="#Optimisers.NADAM"><code>Optimisers.NADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NADAM(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ">NADAM</a> is a Nesterov variant of ADAM. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L354-L367">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ADAMW" href="#Optimisers.ADAMW"><code>Optimisers.ADAMW</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ADAMW(η = 1f-3, β = (9f-1, 9.99f-1), γ = 0, ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1711.05101">ADAMW</a> is a variant of ADAM fixing (as in repairing) its weight decay regularization.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Weight decay (<code>γ</code>): Decay applied to weights during optimisation.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L390-L404">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.AdaBelief" href="#Optimisers.AdaBelief"><code>Optimisers.AdaBelief</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaBelief(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p>The <a href="https://arxiv.org/abs/2010.07468">AdaBelief</a> optimiser is a variant of the well-known ADAM optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ::Float32</code>): Constant to prevent division by zero                                 (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L408-L421">source</a></section></article><p>In addition to the main course, you may wish to order some of these condiments:</p><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ClipGrad" href="#Optimisers.ClipGrad"><code>Optimisers.ClipGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClipGrad(δ = 10f0)</code></pre><p>Restricts every gradient component to obey <code>-δ ≤ dx[i] ≤ δ</code>.</p><p>See also <a href="#Optimisers.ClipNorm"><code>ClipNorm</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L467-L473">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ClipNorm" href="#Optimisers.ClipNorm"><code>Optimisers.ClipNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClipNorm(ω = 10f0, p = 2; throw = true)</code></pre><p>Scales any gradient array for which <code>norm(dx, p) &gt; ω</code> to stay at this threshold (unless <code>p==0</code>).</p><p>Throws an error if the norm is infinite or <code>NaN</code>, which you can turn off with <code>throw = false</code>.</p><p>See also <a href="#Optimisers.ClipGrad"><code>ClipGrad</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L488-L498">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.WeightDecay" href="#Optimisers.WeightDecay"><code>Optimisers.WeightDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">WeightDecay(γ = 5f-4)</code></pre><p>Decay weights by <span>$γ$</span>, that is, add <code>γ .* x</code> to the gradient <code>x̄</code> which will be subtracted from <code>x</code>.</p><p>Typically composed  with other optimisers as the first transformation in an <a href="#Optimisers.OptimiserChain"><code>OptimiserChain</code></a>. This is equivalent to adding <span>$L_2$</span> regularization with coefficient <span>$γ$</span> to the loss.</p><p><strong>Parameters</strong></p><ul><li>Weight decay (<code>γ</code>): Decay applied to weights during optimisation.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L442-L453">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.OptimiserChain" href="#Optimisers.OptimiserChain"><code>Optimisers.OptimiserChain</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OptimiserChain(opts...)</code></pre><p>Compose a sequence of optimisers so that each <code>opt</code> in <code>opts</code> updates the gradient, in the order specified.</p><p>With an empty sequence, <code>OptimiserChain()</code> is the identity, so <code>update!</code> will subtract the full gradient from the parameters. This is equivalent to <code>Descent(1)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; o = OptimiserChain(ClipGrad(1), Descent(0.1));

julia&gt; m = (zeros(3),);

julia&gt; s = Optimisers.setup(o, m)
(Leaf(OptimiserChain(ClipGrad{Int64}(1), Descent{Float64}(0.1)), [nothing, nothing]),)

julia&gt; Optimisers.update(s, m, ([0.3, 1, 7],))[2]  # clips before discounting
([-0.03, -0.1, -0.1],)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/rules.jl#L518-L540">source</a></section></article><h2 id="Model-Interface"><a class="docs-heading-anchor" href="#Model-Interface">Model Interface</a><a id="Model-Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Interface" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Optimisers.setup" href="#Optimisers.setup"><code>Optimisers.setup</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.setup(rule, model) -&gt; tree</code></pre><p>Initialises the given optimiser for every trainable parameter within the model. Returns a tree of the relevant states, which must be passed to <a href="#Optimisers.update"><code>update</code></a> or <a href="#Optimisers.update!"><code>update!</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Optimisers.setup(Descent(0.1f0), (x = rand(3), y = (true, false), z = tanh))
(x = Leaf(Descent{Float32}(0.1), nothing), y = (nothing, nothing), z = nothing)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/Optimisers.jl#L57-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.update" href="#Optimisers.update"><code>Optimisers.update</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.update(tree, model, gradient) -&gt; (tree, model)</code></pre><p>Uses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from <a href="#Optimisers.setup"><code>setup</code></a>.</p><p>See also <a href="#Optimisers.update!"><code>update!</code></a>, which will be faster for models of ordinary <code>Array</code>s or <code>CuArray</code>s.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x = Float32[1,2,3], y = tanh);

julia&gt; t = Optimisers.setup(Descent(0.1f0), m)
(x = Leaf(Descent{Float32}(0.1), nothing), y = nothing)

julia&gt; g = (x = [1,1,1], y = nothing);  # fake gradient

julia&gt; Optimisers.update(t, m, g)
((x = Leaf(Descent{Float32}(0.1), nothing), y = nothing), (x = Float32[0.9, 1.9, 2.9], y = tanh))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/Optimisers.jl#L72-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.update!" href="#Optimisers.update!"><code>Optimisers.update!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.update!(tree, model, gradient) -&gt; (tree, model)</code></pre><p>Uses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from <a href="#Optimisers.setup"><code>setup</code></a>.</p><p>This is used in exactly the same manner as <a href="#Optimisers.update"><code>update</code></a>, but because it may mutate arrays within the old model (and the old state), it will be faster for models of ordinary <code>Array</code>s or <code>CuArray</code>s. However, you should not rely on the old model being fully updated.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/Optimisers.jl#L96-L106">source</a></section></article><p>Calling <code>Functors.@functor</code> on your model&#39;s layer types by default causes the optimiser to act on all suitable fields. To restrict this, define <code>trainable</code>:</p><article class="docstring"><header><a class="docstring-binding" id="Optimisers.trainable" href="#Optimisers.trainable"><code>Optimisers.trainable</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trainable(x::Layer) -&gt; NamedTuple</code></pre><p>This should be overloaded to make optimisers ignore some fields of every <code>Layer</code>, which would otherwise contain trainable parameters. (Elements such as functions and sizes are always ignored.)</p><p>The default is <code>Functors.children(x)</code>, usually a NamedTuple of all fields, and <code>trainable(x)</code> must contain a subset of these.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/interface.jl#L59-L68">source</a></section></article><p>Such restrictions are also obeyed by this function for flattening a model:</p><article class="docstring"><header><a class="docstring-binding" id="Optimisers.destructure" href="#Optimisers.destructure"><code>Optimisers.destructure</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">destructure(model) -&gt; vector, reconstructor</code></pre><p>Copies all <a href="#Optimisers.trainable"><code>trainable</code></a>, <a href="@ref"><code>isnumeric</code></a> parameters in the model to a vector, and returns also a function which reverses this transformation. Differentiable.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; v, re = destructure((x=[1.0, 2.0], y=(sin, [3 + 4im])))
(ComplexF64[1.0 + 0.0im, 2.0 + 0.0im, 3.0 + 4.0im], Restructure(NamedTuple, ..., 3))

julia&gt; re([3, 5-im, 7+11im])
(x = [3.0, 5.0], y = (sin, ComplexF64[7.0 + 11.0im]))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/destructure.jl#L5-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Restructure" href="#Optimisers.Restructure"><code>Optimisers.Restructure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Restructure(Model, ..., length)</code></pre><p>This is what <a href="#Optimisers.destructure"><code>destructure</code></a> returns, and <code>re(p)</code> will re-build the model with new parameters from vector <code>p</code>. If the model is callable, then <code>re(x, p) == re(p)(x)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; using Flux, Optimisers

julia&gt; _, re = destructure(Dense([1 2; 3 4], [0, 0], sigmoid))
([1, 3, 2, 4, 0, 0], Restructure(Dense, ..., 6))

julia&gt; m = re(-4:1)
Dense(2, 2, σ)      # 6 parameters

julia&gt; m([0.2, 0.3]) ≈ re([0.2, 0.3], -4:1)
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/destructure.jl#L26-L45">source</a></section></article><h2 id="Rule-Definition"><a class="docs-heading-anchor" href="#Rule-Definition">Rule Definition</a><a id="Rule-Definition-1"></a><a class="docs-heading-anchor-permalink" href="#Rule-Definition" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Optimisers.apply!" href="#Optimisers.apply!"><code>Optimisers.apply!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.apply!(rule::RuleType, state, parameters, gradient) -&gt; (state, gradient)</code></pre><p>This defines the action of any optimisation rule. It should return the modified gradient which will be subtracted from the parameters, and the updated state (if any) for use at the next iteration, as a tuple <code>(state, gradient)</code>.</p><p>For efficiency it is free to mutate the old state, but only what is returned will be used. Ideally this should check <code>iswriteable(x)</code>, which the built-in rules do via <a href="#Optimisers.@.."><code>@..</code></a>.</p><p>The initial state is <code>init(rule::RuleType, parameters)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Optimisers.init(Descent(0.1), Float32[1,2,3]) === nothing
true

julia&gt; Optimisers.apply!(Descent(0.1), nothing, Float32[1,2,3], [4,5,6])
(nothing, Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}}(*, ([4, 5, 6], 0.1f0)))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/Optimisers.jl#L16-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.init" href="#Optimisers.init"><code>Optimisers.init</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.init(rule::RuleType, parameters) -&gt; state</code></pre><p>Sets up the initial state for a given optimisation rule, and an array of parameters. This and <a href="#Optimisers.apply!"><code>apply!</code></a> are the two functions which any new optimisation rule must define.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Optimisers.init(Descent(), Float32[1,2,3])  # is `nothing`

julia&gt; Optimisers.init(Momentum(), [1.0, 2.0])
2-element Vector{Float64}:
 0.0
 0.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/Optimisers.jl#L39-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.@.." href="#Optimisers.@.."><code>Optimisers.@..</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">@.. x = x + y</code></pre><p>Sometimes in-place broadcasting macro, for use in <code>apply!</code> rules. If <code>iswriteable(x)</code> then it is just <code>@. x = rhs</code>, but if not, it becomes <code>x = @. rhs</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/interface.jl#L80-L85">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.@lazy" href="#Optimisers.@lazy"><code>Optimisers.@lazy</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">x = @lazy y + z</code></pre><p>Lazy broadcasting macro, for use in <code>apply!</code> rules. It broadcasts like <code>@.</code> but does not materialise, returning a <code>Broadcasted</code> object for later use. Beware that mutation of arguments will affect the result, and that if it is used in two places, work will be done twice.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/2bf0efaf11d49482a02366a207bfff0e9ad40759/src/interface.jl#L97-L104">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.15 on <span class="colophon-date" title="Monday 4 April 2022 04:16">Monday 4 April 2022</span>. Using Julia version 1.6.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
