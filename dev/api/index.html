<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · Optimisers.jl</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://fluxml.ai/Optimisers.jl/stable/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><script src="../../copy.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Optimisers.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#Optimisation-Rules"><span>Optimisation Rules</span></a></li><li><a class="tocitem" href="#Model-Interface"><span>Model Interface</span></a></li><li><a class="tocitem" href="#Rule-Definition"><span>Rule Definition</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Optimisers.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h2 id="Optimisation-Rules"><a class="docs-heading-anchor" href="#Optimisation-Rules">Optimisation Rules</a><a id="Optimisation-Rules-1"></a><a class="docs-heading-anchor-permalink" href="#Optimisation-Rules" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Descent" href="#Optimisers.Descent"><code>Optimisers.Descent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Descent(η = 1f-1)</code></pre><p>Classic gradient descent optimiser with learning rate <code>η</code>. For each parameter <code>p</code> and its gradient <code>dp</code>, this runs <code>p -= η*dp</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L1-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Momentum" href="#Optimisers.Momentum"><code>Optimisers.Momentum</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Momentum(η = 1f-2, ρ = 9f-1)</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L24-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.Nesterov" href="#Optimisers.Nesterov"><code>Optimisers.Nesterov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Nesterov(η = 1f-3, ρ = 9f-1)</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and Nesterov momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Nesterov momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                          prominent direction, in effect dampening oscillations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L50-L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.RMSProp" href="#Optimisers.RMSProp"><code>Optimisers.RMSProp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RMSProp(η = 1f-3, ρ = 9f-1, ϵ = eps(typeof(η)))</code></pre><p>Optimizer using the <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a> algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L77-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ADAM" href="#Optimisers.ADAM"><code>Optimisers.ADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ADAM(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1412.6980">ADAM</a> optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L110-L122">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.RADAM" href="#Optimisers.RADAM"><code>Optimisers.RADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RADAM(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1908.03265">Rectified ADAM</a> optimizer.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L143-L155">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.AdaMax" href="#Optimisers.AdaMax"><code>Optimisers.AdaMax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaMax(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1412.6980">AdaMax</a> is a variant of ADAM based on the ∞-norm.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L184-L196">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.OADAM" href="#Optimisers.OADAM"><code>Optimisers.OADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OADAM(η = 1f-3, β = (5f-1, 9f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1711.00141">OADAM</a> (Optimistic ADAM) is a variant of ADAM adding an &quot;optimistic&quot; term suitable for adversarial training.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L218-L231">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ADAGrad" href="#Optimisers.ADAGrad"><code>Optimisers.ADAGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ADAGrad(η = 1f-1, ϵ = eps(typeof(η)))</code></pre><p><a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">ADAGrad</a> optimizer. It has parameter specific learning rates based on how frequently it is updated. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L255-L267">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ADADelta" href="#Optimisers.ADADelta"><code>Optimisers.ADADelta</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ADADelta(ρ = 9f-1, ϵ = eps(typeof(ρ)))</code></pre><p><a href="https://arxiv.org/abs/1212.5701">ADADelta</a> is a version of ADAGrad adapting its learning rate based on a window of past gradient updates. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Rho (<code>ρ</code>): Factor by which the gradient is decayed at each time step.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L286-L297">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.AMSGrad" href="#Optimisers.AMSGrad"><code>Optimisers.AMSGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AMSGrad(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p>The <a href="https://openreview.net/forum?id=ryQu7f-RZ">AMSGrad</a> version of the ADAM optimiser. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L319-L332">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.NADAM" href="#Optimisers.NADAM"><code>Optimisers.NADAM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NADAM(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p><a href="https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ">NADAM</a> is a Nesterov variant of ADAM. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L356-L369">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ADAMW" href="#Optimisers.ADAMW"><code>Optimisers.ADAMW</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ADAMW(η = 1f-3, β = (9f-1, 9.99f-1), γ = 0, ϵ = eps(typeof(η)))</code></pre><p><a href="https://arxiv.org/abs/1711.05101">ADAMW</a> is a variant of ADAM fixing (as in repairing) its weight decay regularization.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Weight decay (<code>γ</code>): Decay applied to weights during optimisation.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L392-L406">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.AdaBelief" href="#Optimisers.AdaBelief"><code>Optimisers.AdaBelief</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaBelief(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))</code></pre><p>The <a href="https://arxiv.org/abs/2010.07468">AdaBelief</a> optimiser is a variant of the well-known ADAM optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ::Float32</code>): Constant to prevent division by zero                                 (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L410-L423">source</a></section></article><p>In addition to the main course, you may wish to order some of these condiments:</p><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ClipGrad" href="#Optimisers.ClipGrad"><code>Optimisers.ClipGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClipGrad(δ = 10f0)</code></pre><p>Restricts every gradient component to obey <code>-δ ≤ dx[i] ≤ δ</code>.</p><p>See also <a href="#Optimisers.ClipNorm"><code>ClipNorm</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L465-L471">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.ClipNorm" href="#Optimisers.ClipNorm"><code>Optimisers.ClipNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClipNorm(ω = 10f0, p = 2; throw = true)</code></pre><p>Scales any gradient array for which <code>norm(dx, p) &gt; ω</code> to stay at this threshold (unless <code>p==0</code>).</p><p>Throws an error if the norm is infinite or <code>NaN</code>, which you can turn off with <code>throw = false</code>.</p><p>See also <a href="#Optimisers.ClipGrad"><code>ClipGrad</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L486-L496">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.WeightDecay" href="#Optimisers.WeightDecay"><code>Optimisers.WeightDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">WeightDecay(γ = 5f-4)</code></pre><p>Decay weights by <code>γ</code>.</p><p><strong>Parameters</strong></p><ul><li>Weight decay (<code>γ</code>): Decay applied to weights during optimisation.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L444-L451">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.OptimiserChain" href="#Optimisers.OptimiserChain"><code>Optimisers.OptimiserChain</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OptimiserChain(opts...)</code></pre><p>Compose a chain (sequence) of optimisers so that each <code>opt</code> in <code>opts</code> updates the gradient in the order specified.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/rules.jl#L516-L521">source</a></section></article><h2 id="Model-Interface"><a class="docs-heading-anchor" href="#Model-Interface">Model Interface</a><a id="Model-Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Interface" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Optimisers.setup" href="#Optimisers.setup"><code>Optimisers.setup</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.setup(rule, model) -&gt; tree</code></pre><p>Initialises the given optimiser for every trainable parameter within the model. Returns a tree of the relevant states, which must be passed to <a href="#Optimisers.update"><code>update</code></a> or <a href="#Optimisers.update!"><code>update!</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Optimisers.setup(Descent(0.1f0), (x = rand(3), y = (true, false), z = tanh))
(x = Leaf(Descent{Float32}(0.1), nothing), y = (nothing, nothing), z = nothing)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/Optimisers.jl#L54-L66">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.update" href="#Optimisers.update"><code>Optimisers.update</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.update(tree, model, gradient) -&gt; (tree, model)</code></pre><p>Uses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from <a href="#Optimisers.setup"><code>setup</code></a>.</p><p>See also <a href="#Optimisers.update!"><code>update!</code></a>, which will be faster for models of ordinary <code>Array</code>s or <code>CuArray</code>s.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x = Float32[1,2,3], y = tanh);

julia&gt; t = Optimisers.setup(Descent(0.1f0), m)
(x = Leaf(Descent{Float32}(0.1), nothing), y = nothing)

julia&gt; g = (x = [1,1,1], y = nothing);  # fake gradient

julia&gt; Optimisers.update(t, m, g)
((x = Leaf(Descent{Float32}(0.1), nothing), y = nothing), (x = Float32[0.9, 1.9, 2.9], y = tanh))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/Optimisers.jl#L69-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.update!" href="#Optimisers.update!"><code>Optimisers.update!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.update!(tree, model, gradient) -&gt; (tree, model)</code></pre><p>Uses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from <a href="#Optimisers.setup"><code>setup</code></a>.</p><p>This is used in exactly the same manner as <a href="#Optimisers.update"><code>update</code></a>, but because it may mutate arrays within the old model (and the old state), it will be faster for models of ordinary <code>Array</code>s or <code>CuArray</code>s. However, you should not rely on the old model being fully updated.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/Optimisers.jl#L93-L103">source</a></section></article><p>Calling <code>Functors.@functor</code> on your model&#39;s layer types by default causes the optimiser to act on all suitable fields. To restrict this, define <code>trainable</code>:</p><article class="docstring"><header><a class="docstring-binding" id="Optimisers.trainable" href="#Optimisers.trainable"><code>Optimisers.trainable</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trainable(x::Layer) -&gt; NamedTuple</code></pre><p>This should be overloaded to make optimisers ignore some fields of every <code>Layer</code>, which would otherwise contain trainable parameters. (Elements such as functions and sizes are always ignored.)</p><p>The default is <code>Functors.children(x)</code>, usually a NamedTuple of all fields, and <code>trainable(x)</code> must contain a subset of these.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/interface.jl#L59-L68">source</a></section></article><h2 id="Rule-Definition"><a class="docs-heading-anchor" href="#Rule-Definition">Rule Definition</a><a id="Rule-Definition-1"></a><a class="docs-heading-anchor-permalink" href="#Rule-Definition" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Optimisers.apply!" href="#Optimisers.apply!"><code>Optimisers.apply!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.apply!(rule::RuleType, state, parameters, gradient) -&gt; (state, gradient)</code></pre><p>This defines the action of any optimisation rule. It should return the modified gradient which will be subtracted from the parameters, and the updated state (if any) for use at the next iteration, as a tuple <code>(state, gradient)</code>.</p><p>For efficiency it is free to mutate the old state, but only what is returned will be used. Ideally this should check <code>iswriteable(x)</code>, which the built-in rules do via <a href="#Optimisers.@.."><code>@..</code></a>.</p><p>The initial state is <code>init(rule::RuleType, parameters)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Optimisers.init(Descent(0.1), [1,2,3]) === nothing
true

julia&gt; Optimisers.apply!(Descent(0.1), nothing, [1,2,3], [4,5,6])
(nothing, Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}}(*, ([4, 5, 6], 0.1)))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/Optimisers.jl#L13-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.init" href="#Optimisers.init"><code>Optimisers.init</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.init(rule::RuleType, parameters) -&gt; state</code></pre><p>Sets up the initial state for a given optimisation rule, and an array of parameters. This and <a href="#Optimisers.apply!"><code>apply!</code></a> are the two functions which any new optimisation rule must define.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Optimisers.init(Descent(), [1,2,3])  # is `nothing`

julia&gt; Optimisers.init(Momentum(), [1.0, 2.0])
2-element Vector{Float64}:
 0.0
 0.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/Optimisers.jl#L36-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.@.." href="#Optimisers.@.."><code>Optimisers.@..</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">@.. x = x + y
@.. x + y / z</code></pre><p>Magic broadcasting macro, for use in <code>apply!</code> rules:</p><ul><li>Applied to assignment <code>x = ...</code> it is like <code>@.</code> unless <code>!iswriteable(x)</code>, in which case it ignores <code>x</code>, and applies <code>@.</code> on the right.</li><li>Applied to other expressions, it broadcasts like <code>@.</code> but does not materialise, returning a <code>Broadcasted</code> object for later use.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/7c778bd7c9f220eafca70f6caf3ce7fa13bebf99/src/interface.jl#L79-L88">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.12 on <span class="colophon-date" title="Sunday 30 January 2022 04:18">Sunday 30 January 2022</span>. Using Julia version 1.6.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
