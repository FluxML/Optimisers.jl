var documenterSearchIndex = {"docs":
[{"location":"api/#Optimisation-Rules","page":"API","title":"Optimisation Rules","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Optimisers.Descent\nOptimisers.Momentum\nOptimisers.Nesterov\nOptimisers.RMSProp\nOptimisers.ADAM\nOptimisers.RADAM\nOptimisers.AdaMax\nOptimisers.OADAM\nOptimisers.ADAGrad\nOptimisers.ADADelta\nOptimisers.AMSGrad\nOptimisers.NADAM\nOptimisers.ADAMW\nOptimisers.AdaBelief","category":"page"},{"location":"api/#Optimisers.Descent","page":"API","title":"Optimisers.Descent","text":"Descent(η = 1f-1)\n\nClassic gradient descent optimiser with learning rate η. For each parameter p and its gradient dp, this runs p -= η*dp.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.Momentum","page":"API","title":"Optimisers.Momentum","text":"Momentum(η = 1f-2, ρ = 9f-1)\n\nGradient descent optimizer with learning rate η and momentum ρ.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nMomentum (ρ): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.Nesterov","page":"API","title":"Optimisers.Nesterov","text":"Nesterov(η = 1f-3, ρ = 9f-1)\n\nGradient descent optimizer with learning rate η and Nesterov momentum ρ.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nNesterov momentum (ρ): Controls the acceleration of gradient descent in the                          prominent direction, in effect dampening oscillations.\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.RMSProp","page":"API","title":"Optimisers.RMSProp","text":"RMSProp(η = 1f-3, ρ = 9f-1, ϵ = eps(typeof(η)))\n\nOptimizer using the RMSProp algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don't need tuning.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nMomentum (ρ): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.ADAM","page":"API","title":"Optimisers.ADAM","text":"ADAM(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))\n\nADAM optimiser.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.RADAM","page":"API","title":"Optimisers.RADAM","text":"RADAM(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))\n\nRectified ADAM optimizer.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.AdaMax","page":"API","title":"Optimisers.AdaMax","text":"AdaMax(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))\n\nAdaMax is a variant of ADAM based on the ∞-norm.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.OADAM","page":"API","title":"Optimisers.OADAM","text":"OADAM(η = 1f-3, β = (5f-1, 9f-1), ϵ = eps(typeof(η)))\n\nOADAM (Optimistic ADAM) is a variant of ADAM adding an \"optimistic\" term suitable for adversarial training.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.ADAGrad","page":"API","title":"Optimisers.ADAGrad","text":"ADAGrad(η = 1f-1, ϵ = eps(typeof(η)))\n\nADAGrad optimizer. It has parameter specific learning rates based on how frequently it is updated. Parameters don't need tuning.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.ADADelta","page":"API","title":"Optimisers.ADADelta","text":"ADADelta(ρ = 9f-1, ϵ = eps(typeof(ρ)))\n\nADADelta is a version of ADAGrad adapting its learning rate based on a window of past gradient updates. Parameters don't need tuning.\n\nParameters\n\nRho (ρ): Factor by which the gradient is decayed at each time step.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.AMSGrad","page":"API","title":"Optimisers.AMSGrad","text":"AMSGrad(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))\n\nThe AMSGrad version of the ADAM optimiser. Parameters don't need tuning.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.NADAM","page":"API","title":"Optimisers.NADAM","text":"NADAM(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))\n\nNADAM is a Nesterov variant of ADAM. Parameters don't need tuning.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.ADAMW","page":"API","title":"Optimisers.ADAMW","text":"ADAMW(η = 1f-3, β = (9f-1, 9.99f-1), γ = 0, ϵ = eps(typeof(η)))\n\nADAMW is a variant of ADAM fixing (as in repairing) its weight decay regularization.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nWeight decay (γ): Decay applied to weights during optimisation.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"function"},{"location":"api/#Optimisers.AdaBelief","page":"API","title":"Optimisers.AdaBelief","text":"AdaBelief(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))\n\nThe AdaBelief optimiser is a variant of the well-known ADAM optimiser.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ::Float32): Constant to prevent division by zero                                 (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"In addition to the main course, you may wish to order some of these condiments:","category":"page"},{"location":"api/","page":"API","title":"API","text":"Optimisers.ClipGrad\nOptimisers.ClipNorm\nOptimisers.WeightDecay\nOptimisers.OptimiserChain","category":"page"},{"location":"api/#Optimisers.ClipGrad","page":"API","title":"Optimisers.ClipGrad","text":"ClipGrad(δ = 10f0)\n\nRestricts every gradient component to obey -δ ≤ dx[i] ≤ δ.\n\nSee also ClipNorm.\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.ClipNorm","page":"API","title":"Optimisers.ClipNorm","text":"ClipNorm(ω = 10f0, p = 2; throw = true)\n\nScales any gradient array for which norm(dx, p) > ω to stay at this threshold (unless p==0).\n\nThrows an error if the norm is infinite or NaN, which you can turn off with throw = false.\n\nSee also ClipGrad.\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.WeightDecay","page":"API","title":"Optimisers.WeightDecay","text":"WeightDecay(γ = 5f-4)\n\nDecay weights by γ.\n\nParameters\n\nWeight decay (γ): Decay applied to weights during optimisation.\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.OptimiserChain","page":"API","title":"Optimisers.OptimiserChain","text":"OptimiserChain(opts...)\n\nCompose a chain (sequence) of optimisers so that each opt in opts updates the gradient in the order specified.\n\n\n\n\n\n","category":"type"},{"location":"api/#Model-Interface","page":"API","title":"Model Interface","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Optimisers.setup\nOptimisers.update\nOptimisers.update!","category":"page"},{"location":"api/#Optimisers.setup","page":"API","title":"Optimisers.setup","text":"Optimisers.setup(rule, model) -> tree\n\nInitialises the given optimiser for every trainable parameter within the model. Returns a tree of the relevant states, which must be passed to update or update!.\n\nExample\n\njulia> Optimisers.setup(Descent(0.1f0), (x = rand(3), y = (true, false), z = tanh))\n(x = Leaf(Descent{Float32}(0.1), nothing), y = (nothing, nothing), z = nothing)\n\n\n\n\n\n","category":"function"},{"location":"api/#Optimisers.update","page":"API","title":"Optimisers.update","text":"Optimisers.update(tree, model, gradient) -> (tree, model)\n\nUses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from setup.\n\nSee also update!, which will be faster for models of ordinary Arrays or CuArrays.\n\nExample\n\njulia> m = (x = Float32[1,2,3], y = tanh);\n\njulia> t = Optimisers.setup(Descent(0.1f0), m)\n(x = Leaf(Descent{Float32}(0.1), nothing), y = nothing)\n\njulia> g = (x = [1,1,1], y = nothing);  # fake gradient\n\njulia> Optimisers.update(t, m, g)\n((x = Leaf(Descent{Float32}(0.1), nothing), y = nothing), (x = Float32[0.9, 1.9, 2.9], y = tanh))\n\n\n\n\n\n","category":"function"},{"location":"api/#Optimisers.update!","page":"API","title":"Optimisers.update!","text":"Optimisers.update!(tree, model, gradient) -> (tree, model)\n\nUses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from setup.\n\nThis is used in exactly the same manner as update, but because it may mutate arrays within the old model (and the old state), it will be faster for models of ordinary Arrays or CuArrays. However, you should not rely on the old model being fully updated.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"Calling Functors.@functor on your model's layer types by default causes the optimiser to act on all suitable fields. To restrict this, define trainable:","category":"page"},{"location":"api/","page":"API","title":"API","text":"Optimisers.trainable","category":"page"},{"location":"api/#Optimisers.trainable","page":"API","title":"Optimisers.trainable","text":"trainable(x::Layer) -> NamedTuple\n\nThis should be overloaded to make optimisers ignore some fields of every Layer, which would otherwise contain trainable parameters. (Elements such as functions and sizes are always ignored.)\n\nThe default is Functors.children(x), usually a NamedTuple of all fields, and trainable(x) must contain a subset of these.\n\n\n\n\n\n","category":"function"},{"location":"api/#Rule-Definition","page":"API","title":"Rule Definition","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Optimisers.apply!\nOptimisers.init\nOptimisers.@..","category":"page"},{"location":"api/#Optimisers.apply!","page":"API","title":"Optimisers.apply!","text":"Optimisers.apply!(rule::RuleType, state, parameters, gradient) -> (state, gradient)\n\nThis defines the action of any optimisation rule. It should return the modified gradient which will be subtracted from the parameters, and the updated state (if any) for use at the next iteration, as a tuple (state, gradient).\n\nFor efficiency it is free to mutate the old state, but only what is returned will be used. Ideally this should check iswriteable(x), which the built-in rules do via @...\n\nThe initial state is init(rule::RuleType, parameters).\n\nExample\n\njulia> Optimisers.init(Descent(0.1), [1,2,3]) === nothing\ntrue\n\njulia> Optimisers.apply!(Descent(0.1), nothing, [1,2,3], [4,5,6])\n(nothing, Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}}(*, ([4, 5, 6], 0.1)))\n\n\n\n\n\n","category":"function"},{"location":"api/#Optimisers.init","page":"API","title":"Optimisers.init","text":"Optimisers.init(rule::RuleType, parameters) -> state\n\nSets up the initial state for a given optimisation rule, and an array of parameters. This and apply! are the two functions which any new optimisation rule must define.\n\nExamples\n\njulia> Optimisers.init(Descent(), [1,2,3])  # is `nothing`\n\njulia> Optimisers.init(Momentum(), [1.0, 2.0])\n2-element Vector{Float64}:\n 0.0\n 0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#Optimisers.@..","page":"API","title":"Optimisers.@..","text":"@.. x = x + y\n@.. x + y / z\n\nMagic broadcasting macro, for use in apply! rules:\n\nApplied to assignment x = ... it is like @. unless !iswriteable(x), in which case it ignores x, and applies @. on the right.\nApplied to other expressions, it broadcasts like @. but does not materialise, returning a Broadcasted object for later use.\n\n\n\n\n\n","category":"macro"},{"location":"#Optimisers.jl","page":"Home","title":"Optimisers.jl","text":"","category":"section"},{"location":"#Define-an-Optimiser","page":"Home","title":"Define an Optimiser","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A new optimiser must overload two functions, apply! and init:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Define a container to hold any optimiser specific parameters (if any):\nstruct DecayDescent{T}\n  η::T\nend\n\n# Define an `apply!` rule which encodes how the gradients will be used to\n# update the parameters:\nfunction Optimisers.apply!(o::DecayDescent, state, x, x̄)\n  newx̄ = (o.η / √state) .* x̄\n  nextstate = state + 1\n  return nextstate, newx̄\nend\n\n# Define the function which sets up the initial state (if any):\nOptimisers.init(o::DecayDescent, x::AbstractArray) = 1","category":"page"},{"location":"","page":"Home","title":"Home","text":"The parameters will be immediately updated to x .- newx̄, while nextstate is caried to the next iteration.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Notice that the state is handled separately from the optimiser itself. This is a key design principle and allows users to manage their own state explicitly.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It of course also makes it easier to store the state.","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To apply such an optimiser to a whole model, setup builds a tree containing any initial state for every trainable array. Then at each step, update uses this and the gradient to adjust the model:","category":"page"},{"location":"","page":"Home","title":"Home","text":"\nusing Flux, Metalhead, Optimisers\n\no = Optimisers.ADAM() # define an ADAM optimiser with default settings\nst = Optimisers.setup(o, m)  # initialize the optimiser before using it\n\nmodel = ResNet18() # define a model to train on\nip = rand(Float32, 224, 224, 3, 1) # dummy data\n\nm̄, _ = gradient(model, ip) do m, x # calculate the gradients\n  sum(m(x)) # dummy loss function\nend\n\nst, mnew = Optimisers.update(st, m, m̄)\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"Notice that a completely new instance of the model is returned. Internally, this is handled by Functors.jl, where we do a walk over the tree formed by the model and update the parameters using the gradients. Optimisers can work with different forms of gradients, but most likely use case are the gradients as returned by Zygote.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"There is also Optimisers.update! which similarly returns a new model and new state, but is free to mutate arrays within the old one for efficiency. The method of apply! you write is likewise free to mutate arrays within its state; they are defensively copied when this rule is used with update.","category":"page"}]
}
