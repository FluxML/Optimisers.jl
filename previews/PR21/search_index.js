var documenterSearchIndex = {"docs":
[{"location":"api/","page":"API","title":"API","text":"Optimisers.Descent\nOptimisers.Momentum\nOptimisers.Nesterov\nOptimisers.RMSProp\nOptimisers.ADAM\nOptimisers.RADAM\nOptimisers.AdaMax\nOptimisers.OADAM\nOptimisers.ADAGrad\nOptimisers.ADADelta\nOptimisers.AMSGrad\nOptimisers.NADAM\nOptimisers.ADAMW\nOptimisers.AdaBelief\nOptimisers.weightDecay\nOptimisers.OptimiserChain","category":"page"},{"location":"api/#Optimisers.Descent","page":"API","title":"Optimisers.Descent","text":"Descent(η = 1f-1)\n\nClassic gradient descent optimiser with learning rate η. For each parameter p and its gradient dp, this runs p -= η*dp.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.Momentum","page":"API","title":"Optimisers.Momentum","text":"Momentum(η = 1f-2, ρ = 9f-1)\n\nGradient descent optimizer with learning rate η and momentum ρ.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nMomentum (ρ): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.Nesterov","page":"API","title":"Optimisers.Nesterov","text":"Nesterov(η = 1f-3, ρ = 9f-1)\n\nGradient descent optimizer with learning rate η and Nesterov momentum ρ.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nNesterov momentum (ρ): Controls the acceleration of gradient descent in the                          prominent direction, in effect dampening oscillations.\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.RMSProp","page":"API","title":"Optimisers.RMSProp","text":"RMSProp(η = 1f-3, ρ = 9f-1, ϵ = eps(typeof(η)))\n\nOptimizer using the RMSProp algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don't need tuning.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nMomentum (ρ): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.ADAM","page":"API","title":"Optimisers.ADAM","text":"ADAM(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))\n\nADAM optimiser.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.RADAM","page":"API","title":"Optimisers.RADAM","text":"RADAM(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))\n\nRectified ADAM optimizer.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.AdaMax","page":"API","title":"Optimisers.AdaMax","text":"AdaMax(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))\n\nAdaMax is a variant of ADAM based on the ∞-norm.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.OADAM","page":"API","title":"Optimisers.OADAM","text":"OADAM(η = 1f-3, β = (5f-1, 9f-1), ϵ = eps(typeof(η)))\n\nOADAM (Optimistic ADAM) is a variant of ADAM adding an \"optimistic\" term suitable for adversarial training.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.ADAGrad","page":"API","title":"Optimisers.ADAGrad","text":"ADAGrad(η = 1f-1, ϵ = eps(typeof(η)))\n\nADAGrad optimizer. It has parameter specific learning rates based on how frequently it is updated. Parameters don't need tuning.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.ADADelta","page":"API","title":"Optimisers.ADADelta","text":"ADADelta(ρ = 9f-1, ϵ = eps(typeof(ρ)))\n\nADADelta is a version of ADAGrad adapting its learning rate based on a window of past gradient updates. Parameters don't need tuning.\n\nParameters\n\nRho (ρ): Factor by which the gradient is decayed at each time step.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.AMSGrad","page":"API","title":"Optimisers.AMSGrad","text":"AMSGrad(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))\n\nThe AMSGrad version of the ADAM optimiser. Parameters don't need tuning.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.NADAM","page":"API","title":"Optimisers.NADAM","text":"NADAM(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))\n\nNADAM is a Nesterov variant of ADAM. Parameters don't need tuning.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.ADAMW","page":"API","title":"Optimisers.ADAMW","text":"ADAMW(η = 1f-3, β = (9f-1, 9.99f-1), γ = 0, ϵ = eps(typeof(η)))\n\nADAMW is a variant of ADAM fixing (as in repairing) its weight decay regularization.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nWeight decay (γ): Decay applied to weights during optimisation.\nMachine epsilon (ϵ): Constant to prevent division by zero                        (no need to change default)\n\n\n\n\n\n","category":"function"},{"location":"api/#Optimisers.AdaBelief","page":"API","title":"Optimisers.AdaBelief","text":"AdaBelief(η = 1f-3, β = (9f-1, 9.99f-1), ϵ = eps(typeof(η)))\n\nThe AdaBelief optimiser is a variant of the well-known ADAM optimiser.\n\nParameters\n\nLearning rate (η): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (β::Tuple): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\nMachine epsilon (ϵ::Float32): Constant to prevent division by zero                                 (no need to change default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Optimisers.OptimiserChain","page":"API","title":"Optimisers.OptimiserChain","text":"OptimiserChain(opts...)\n\nCompose a chain (sequence) of optimisers so that each opt in opts updates the gradient in the order specified.\n\n\n\n\n\n","category":"type"},{"location":"optimisers/#Optimisers","page":"Home","title":"Optimisers","text":"","category":"section"},{"location":"optimisers/","page":"Home","title":"Home","text":"Consider a basic linear regression. There are a few ingredients we need to get started.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"We create some dummy data, calculate a loss, and backpropagate to calculate gradients for the parameters W and b.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"using Flux\n\nW = rand(2, 5)\nb = rand(2)\n\npredict(x) = (W * x) .+ b\nloss(x, y) = sum((predict(x) .- y).^2)\n\nx, y = rand(5), rand(2) # Dummy data\nl = loss(x, y) # ~ 3\n\nθ = params(W, b)\ngrads = gradient(() -> loss(x, y), θ)","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"We want to update each parameter, using the gradient, in order to improve (reduce) the loss. Here's one way to do that:","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"using Optimisers: update\n\nη = 0.1 # Learning Rate\nfor p in (W, b)\n  p, _ = Optimisers.update(p, η * grads[p], nothing)\nend","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"Running this will alter the parameters W and b and our loss should go down. Flux provides a more general way to do optimiser updates like this.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"opt = Descent(0.1) # Gradient descent with learning rate 0.1\n\nfor p in (W, b)\n  p, _ = update(opt, p, grads[p], nothing)\nend","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"An optimiser update accepts a parameter and a gradient, and updates the parameter according to the chosen rule using the state of the optimiser. We can also pass opt to our training loop, which will update all parameters of the model in a loop. However, we can now easily replace Descent with a more advanced optimiser such as ADAM.","category":"page"},{"location":"optimisers/#Optimiser-Reference","page":"Home","title":"Optimiser Reference","text":"","category":"section"},{"location":"optimisers/","page":"Home","title":"Home","text":"All optimisers return an object that, when passed to train!, will update the parameters passed to it.","category":"page"},{"location":"optimisers/#Optimiser-Interface","page":"Home","title":"Optimiser Interface","text":"","category":"section"},{"location":"optimisers/","page":"Home","title":"Home","text":"Flux's optimisers are built around a struct that holds all the optimiser parameters along with a definition of how to apply the update rule associated with it. We do this via the apply function which takes the optimiser as the first argument followed by the parameter and its corresponding gradient.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"In this manner Flux also allows one to create custom optimisers to be used seamlessly. Let's work this with a simple example.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"mutable struct Momentum\n  eta\n  rho\n  velocity\nend\n\nMomentum(eta::Real, rho::Real) = Momentum(eta, rho, IdDict())","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"The Momentum type will act as our optimiser in this case. Notice that we have added all the parameters as fields, along with the velocity which we will use as our state dictionary. Each parameter in our models will get an entry in there. We can now define the rule applied when this optimiser is invoked.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"function Flux.Optimise.apply!(o::Momentum, x, Δ)\n  η, ρ = o.eta, o.rho\n  v = get!(o.velocity, x, zero(x))::typeof(x)\n  @. v = ρ * v - η * Δ\n  @. Δ = -v\nend","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"This is the basic definition of a Momentum update rule given by:","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"v = ρ * v - η * Δ\nw = w - v","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"The apply! defines the update rules for an optimiser opt, given the parameters and gradients. It returns the updated gradients. Here, every parameter x is retrieved from the running state v and subsequently updates the state of the optimiser.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"Flux internally calls on this function via the update! function. It shares the API with apply! but ensures that multiple parameters are handled gracefully.","category":"page"},{"location":"optimisers/#Composing-Optimisers","page":"Home","title":"Composing Optimisers","text":"","category":"section"},{"location":"optimisers/","page":"Home","title":"Home","text":"Flux defines a special kind of optimiser simply called Optimiser which takes in arbitrary optimisers as input. Its behaviour is similar to the usual optimisers, but differs in that it acts by calling the optimisers listed in it sequentially. Each optimiser produces a modified gradient that will be fed into the next, and the resultant update will be applied to the parameter as usual. A classic use case is where adding decays is desirable. Flux defines some basic decays including ExpDecay, InvDecay etc.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"opt = Optimiser(ExpDecay(1, 0.1, 1000, 1e-4), Descent())","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"Here we apply exponential decay to the Descent optimiser. The defaults of ExpDecay say that its learning rate will be decayed every 1000 steps. It is then applied like any optimiser.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"w = randn(10, 10)\nw1 = randn(10,10)\nps = Params([w, w1])\n\nloss(x) = Flux.Losses.mse(w * x, w1 * x)\n\nloss(rand(10)) # around 9\n\nfor t = 1:10^5\n  θ = Params([w, w1])\n  θ̄ = gradient(() -> loss(rand(10)), θ)\n  Flux.Optimise.update!(opt, θ, θ̄)\nend\n\nloss(rand(10)) # around 0.9","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"In this manner it is possible to compose optimisers for some added flexibility.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"Flux.Optimise.Optimiser","category":"page"},{"location":"optimisers/#Scheduling-Optimisers","page":"Home","title":"Scheduling Optimisers","text":"","category":"section"},{"location":"optimisers/","page":"Home","title":"Home","text":"In practice, it is fairly common to schedule the learning rate of an optimiser to obtain faster convergence. There are a variety of popular scheduling policies, and you can find implementations of them in ParameterSchedulers.jl. The documentation for ParameterSchedulers.jl provides a more detailed overview of the different scheduling policies, and how to use them with Flux optimizers. Below, we provide a brief snippet illustrating a cosine annealing schedule with a momentum optimiser.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"First, we import ParameterSchedulers.jl and initalize a cosine annealing schedule to varying the learning rate between 1e-4 and 1e-2 every 10 steps. We also create a new Momentum optimiser.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"using ParameterSchedulers\n\nopt = Momentum()\nschedule = Cos(λ0 = 1e-4, λ1 = 1e-2, period = 10)\nfor (eta, epoch) in zip(schedule, 1:100)\n  opt.eta = eta\n  # your training code here\nend","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"schedule can also be indexed (e.g. schedule(100)) or iterated like any iterator in Julia.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"ParameterSchedulers.jl schedules are stateless (they don't store their iteration state). If you want a stateful schedule, you can use ParameterSchedulers.Stateful:","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"using ParameterSchedulers: Stateful, next!\n\nschedule = Stateful(Cos(λ0 = 1e-4, λ1 = 1e-2, period = 10))\nfor epoch in 1:100\n  opt.eta = next!(schedule)\n  # your training code here\nend","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"ParameterSchedulers.jl allows for many more scheduling policies including arbitrary functions, looping any function with a given period, or sequences of many schedules. See the ParameterSchedulers.jl documentation for more info.","category":"page"},{"location":"optimisers/#Decays","page":"Home","title":"Decays","text":"","category":"section"},{"location":"optimisers/","page":"Home","title":"Home","text":"Similar to optimisers, Flux also defines some simple decays that can be used in conjunction with other optimisers, or standalone.","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"ExpDecay\nInvDecay\nWeightDecay","category":"page"},{"location":"optimisers/#Optimisers.WeightDecay","page":"Home","title":"Optimisers.WeightDecay","text":"WeightDecay(γ = 5f-4)\n\nDecay weights by γ.\n\nParameters\n\nWeight decay (γ): Decay applied to weights during optimisation.\n\n\n\n\n\n","category":"type"},{"location":"optimisers/#Gradient-Clipping","page":"Home","title":"Gradient Clipping","text":"","category":"section"},{"location":"optimisers/","page":"Home","title":"Home","text":"Gradient clipping is useful for training recurrent neural networks, which have a tendency to suffer from the exploding gradient problem. An example usage is","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"opt = Optimiser(ClipValue(1e-3), ADAM(1e-3))","category":"page"},{"location":"optimisers/","page":"Home","title":"Home","text":"ClipValue\nClipNorm","category":"page"},{"location":"training/#Training","page":"Training","title":"Training","text":"","category":"section"},{"location":"training/","page":"Training","title":"Training","text":"To actually train a model we need four things:","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"A objective function, that evaluates how well a model is doing given some input data.\nThe trainable parameters of the model.\nA collection of data points that will be provided to the objective function.\nAn optimiser that will update the model parameters appropriately.","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"Training a model is typically an iterative process, where we go over the data set, calculate the objective function over the datapoints, and optimise that. This can be visualised in the form of a simple loop.","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"for d in datapoints\n\n  # `d` should produce a collection of arguments\n  # to the loss function\n\n  # Calculate the gradients of the parameters\n  # with respect to the loss function\n  grads = Flux.gradient(parameters) do\n    loss(d...)\n  end\n\n  # Update the parameters based on the chosen\n  # optimiser (opt)\n  updated_parameters, updated_state = Optimisers.update(opt, state, parameters, grads)\nend","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"To make it easy, Flux defines train!:","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"Flux.Optimise.train!","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"There are plenty of examples in the model zoo, and more information can be found on Custom Training Loops.","category":"page"},{"location":"training/#Loss-Functions","page":"Training","title":"Loss Functions","text":"","category":"section"},{"location":"training/","page":"Training","title":"Training","text":"The objective function must return a number representing how far the model is from its target – the loss of the model. The loss function that we defined in basics will work as an objective. In addition to custom losses, model can be trained in conjuction with the commonly used losses that are grouped under the Flux.Losses module. We can also define an objective in terms of some model:","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"m = Chain(\n  Dense(784, 32, σ),\n  Dense(32, 10), softmax)\n\nloss(x, y) = Flux.Losses.mse(m(x), y)\nps = Flux.params(m)\n\n# later\nFlux.train!(loss, ps, data, opt)","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"The objective will almost always be defined in terms of some cost function that measures the distance of the prediction m(x) from the target y. Flux has several of these built in, like mse for mean squared error or crossentropy for cross entropy loss, but you can calculate it however you want. For a list of all built-in loss functions, check out the losses reference.","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"At first glance it may seem strange that the model that we want to train is not part of the input arguments of Flux.train! too. However the target of the optimizer is not the model itself, but the objective function that represents the departure between modelled and observed data. In other words, the model is implicitly defined in the objective function, and there is no need to give it explicitly. Passing the objective function instead of the model and a cost function separately provides more flexibility, and the possibility of optimizing the calculations.","category":"page"},{"location":"training/#Model-parameters","page":"Training","title":"Model parameters","text":"","category":"section"},{"location":"training/","page":"Training","title":"Training","text":"The model to be trained must have a set of tracked parameters that are used to calculate the gradients of the objective function. In the basics section it is explained how to create models with such parameters. The second argument of the function Flux.train! must be an object containing those parameters, which can be obtained from a model m as params(m).","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"Such an object contains a reference to the model's parameters, not a copy, such that after their training, the model behaves according to their updated values.","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"Handling all the parameters on a layer by layer basis is explained in the Layer Helpers section. Also, for freezing model parameters, see the Advanced Usage Guide.","category":"page"},{"location":"training/#Datasets","page":"Training","title":"Datasets","text":"","category":"section"},{"location":"training/","page":"Training","title":"Training","text":"The data argument of train! provides a collection of data to train with (usually a set of inputs x and target outputs y). For example, here's a dummy dataset with only one data point:","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"x = rand(784)\ny = rand(10)\ndata = [(x, y)]","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"Flux.train! will call loss(x, y), calculate gradients, update the weights and then move on to the next data point if there is one. We can train the model on the same data three times:","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"data = [(x, y), (x, y), (x, y)]\n# Or equivalently\nusing IterTools: ncycle\ndata = ncycle([(x, y)], 3)","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"It's common to load the xs and ys separately. In this case you can use zip:","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"xs = [rand(784), rand(784), rand(784)]\nys = [rand( 10), rand( 10), rand( 10)]\ndata = zip(xs, ys)","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"Training data can be conveniently  partitioned for mini-batch training using the Flux.Data.DataLoader type:","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"X = rand(28, 28, 60000)\nY = rand(0:9, 60000)\ndata = DataLoader(X, Y, batchsize=128) ","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"Note that, by default, train! only loops over the data once (a single \"epoch\"). A convenient way to run multiple epochs from the REPL is provided by @epochs.","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"julia> using Flux: @epochs\n\njulia> @epochs 2 println(\"hello\")\nINFO: Epoch 1\nhello\nINFO: Epoch 2\nhello\n\njulia> @epochs 2 Flux.train!(...)\n# Train for two epochs","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"Flux.@epochs","category":"page"},{"location":"training/#Callbacks","page":"Training","title":"Callbacks","text":"","category":"section"},{"location":"training/","page":"Training","title":"Training","text":"train! takes an additional argument, cb, that's used for callbacks so that you can observe the training process. For example:","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"train!(objective, ps, data, opt, cb = () -> println(\"training\"))","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"Callbacks are called for every batch of training data. You can slow this down using Flux.throttle(f, timeout) which prevents f from being called more than once every timeout seconds.","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"A more typical callback might look like this:","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"test_x, test_y = # ... create single batch of test data ...\nevalcb() = @show(loss(test_x, test_y))\nthrottled_cb = throttle(evalcb, 5)\nFlux.@epochs 20 Flux.train!(objective, ps, data, opt, cb = throttled_cb)","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"Calling Flux.stop() in a callback will exit the training loop early.","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"cb = function ()\n  accuracy() > 0.9 && Flux.stop()\nend","category":"page"},{"location":"training/#Custom-Training-loops","page":"Training","title":"Custom Training loops","text":"","category":"section"},{"location":"training/","page":"Training","title":"Training","text":"The Flux.train! function can be very convenient, especially for simple problems. Its also very flexible with the use of callbacks. But for some problems its much cleaner to write your own custom training loop. An example follows that works similar to the default Flux.train but with no callbacks. You don't need callbacks if you just code the calls to your functions directly into the loop. E.g. in the places marked with comments.","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"function my_custom_train!(loss, ps, data, opt)\n  # training_loss is declared local so it will be available for logging outside the gradient calculation.\n  local training_loss\n  ps = Params(ps)\n  for d in data\n    gs = gradient(ps) do\n      training_loss = loss(d...)\n      # Code inserted here will be differentiated, unless you need that gradient information\n      # it is better to do the work outside this block.\n      return training_loss\n    end\n    # Insert whatever code you want here that needs training_loss, e.g. logging.\n    # logging_callback(training_loss)\n    # Insert what ever code you want here that needs gradient.\n    # E.g. logging with TensorBoardLogger.jl as histogram so you can see if it is becoming huge.\n    ps, st = update(opt, st, ps, gs)\n    # Here you might like to check validation set accuracy, and break out to do early stopping.\n  end\nend","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"You could simplify this further, for example by hard-coding in the loss function.","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"Another possibility is to use Zygote.pullback to access the training loss and the gradient simultaneously.","category":"page"},{"location":"training/","page":"Training","title":"Training","text":"function my_custom_train!(loss, ps, data, opt)\n  ps = Params(ps)\n  for d in data\n    # back is a method that computes the product of the gradient so far with its argument.\n    train_loss, back = Zygote.pullback(() -> loss(d...), ps)\n    # Insert whatever code you want here that needs training_loss, e.g. logging.\n    # logging_callback(training_loss)\n    # Apply back() to the correct type of 1.0 to get the gradient of loss.\n    gs = back(one(train_loss))\n    # Insert what ever code you want here that needs gradient.\n    # E.g. logging with TensorBoardLogger.jl as histogram so you can see if it is becoming huge.\n    ps, _ = update(opt, st, ps, gs)\n    # Here you might like to check validation set accuracy, and break out to do early stopping.\n  end\nend","category":"page"},{"location":"#Optimisers.jl","page":"Optimisers.jl","title":"Optimisers.jl","text":"","category":"section"}]
}
