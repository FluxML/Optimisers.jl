<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · Optimisers.jl</title><meta name="title" content="API · Optimisers.jl"/><meta property="og:title" content="API · Optimisers.jl"/><meta property="twitter:title" content="API · Optimisers.jl"/><meta name="description" content="Documentation for Optimisers.jl."/><meta property="og:description" content="Documentation for Optimisers.jl."/><meta property="twitter:description" content="Documentation for Optimisers.jl."/><meta property="og:url" content="https://fluxml.ai/Optimisers.jl/stable/api/"/><meta property="twitter:url" content="https://fluxml.ai/Optimisers.jl/stable/api/"/><link rel="canonical" href="https://fluxml.ai/Optimisers.jl/stable/api/"/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="Optimisers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Optimisers.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#Optimisation-Rules"><span>Optimisation Rules</span></a></li><li><a class="tocitem" href="#Model-Interface"><span>Model Interface</span></a></li><li><a class="tocitem" href="#Rule-Definition"><span>Rule Definition</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FluxML/Optimisers.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FluxML/Optimisers.jl/blob/master/docs/src/api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h2 id="Optimisation-Rules"><a class="docs-heading-anchor" href="#Optimisation-Rules">Optimisation Rules</a><a id="Optimisation-Rules-1"></a><a class="docs-heading-anchor-permalink" href="#Optimisation-Rules" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.Descent" href="#Optimisers.Descent"><code>Optimisers.Descent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Descent(η = 1f-1)
Descent(; eta)</code></pre><p>Classic gradient descent optimiser with learning rate <code>η</code>. For each parameter <code>p</code> and its gradient <code>dp</code>, this runs <code>p -= η*dp</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η == eta</code>): Amount by which gradients are discounted before updating                      the weights.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L9-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.Momentum" href="#Optimisers.Momentum"><code>Optimisers.Momentum</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Momentum(η = 0.01, ρ = 0.9)
Momentum(; [eta, rho])</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η == eta</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ == rho</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L39-L50">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.Nesterov" href="#Optimisers.Nesterov"><code>Optimisers.Nesterov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Nesterov(η = 0.001, ρ = 0.9)</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and Nesterov momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Nesterov momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                          prominent direction, in effect dampening oscillations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L65-L75">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.Rprop" href="#Optimisers.Rprop"><code>Optimisers.Rprop</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Rprop(η = 1f-3, ℓ = (5f-1, 1.2f0), Γ = (1f-6, 50f0))</code></pre><p>Optimizer using the <a href="https://ieeexplore.ieee.org/document/298623">Rprop</a> algorithm. A full-batch learning algorithm that depends only on the sign of the gradient.</p><p><strong>Parameters</strong></p><ul><li><p>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</p></li><li><p>Scaling factors (<code>ℓ::Tuple</code>): Multiplicative increase and decrease factors.</p></li><li><p>Step sizes (<code>Γ::Tuple</code>): Mminimal and maximal allowed step sizes.</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L154-L168">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.RMSProp" href="#Optimisers.RMSProp"><code>Optimisers.RMSProp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RMSProp(η = 0.001, ρ = 0.9, ϵ = 1e-8; centred = false)
RMSProp(; [eta, rho, epsilon, centred])</code></pre><p>Optimizer using the <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a> algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don&#39;t need tuning.</p><p><a href="http://arxiv.org/abs/1308.08500">Centred RMSProp</a> is a variant which normalises gradients by an estimate their variance, instead of their second moment.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η == eta</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ == rho</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.</li><li>Machine epsilon (<code>ϵ == epsilon</code>): Constant to prevent division by zero                        (no need to change default)</li><li>Keyword <code>centred</code> (or <code>centered</code>): Indicates whether to use centred variant                                    of the algorithm.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L92-L113">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.Adam" href="#Optimisers.Adam"><code>Optimisers.Adam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Adam(η = 0.001, β = (0.9, 0.999), ϵ = 1e-8)</code></pre><p><a href="https://arxiv.org/abs/1412.6980">Adam</a> optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L194-L206">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.RAdam" href="#Optimisers.RAdam"><code>Optimisers.RAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RAdam(η = 0.001, β = (0.9, 0.999), ϵ = 1e-8)</code></pre><p><a href="https://arxiv.org/abs/1908.03265">Rectified Adam</a> optimizer.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L255-L267">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AdaMax" href="#Optimisers.AdaMax"><code>Optimisers.AdaMax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaMax(η = 0.001, β = (0.9, 0.999), ϵ = 1e-8)</code></pre><p><a href="https://arxiv.org/abs/1412.6980">AdaMax</a> is a variant of Adam based on the ∞-norm.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L295-L307">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.OAdam" href="#Optimisers.OAdam"><code>Optimisers.OAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OAdam(η = 0.001, β = (0.5, 0.9), ϵ = 1e-8)</code></pre><p><a href="https://arxiv.org/abs/1711.00141">OAdam</a> (Optimistic Adam) is a variant of Adam adding an &quot;optimistic&quot; term suitable for adversarial training.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L327-L340">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AdaGrad" href="#Optimisers.AdaGrad"><code>Optimisers.AdaGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaGrad(η = 0.1, ϵ = 1e-8)</code></pre><p><a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">AdaGrad</a> optimizer. It has parameter specific learning rates based on how frequently it is updated. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L362-L374">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AdaDelta" href="#Optimisers.AdaDelta"><code>Optimisers.AdaDelta</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaDelta(ρ = 0.9, ϵ = 1e-8)</code></pre><p><a href="https://arxiv.org/abs/1212.5701">AdaDelta</a> is a version of AdaGrad adapting its learning rate based on a window of past gradient updates. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Rho (<code>ρ</code>): Factor by which the gradient is decayed at each time step.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L392-L403">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AMSGrad" href="#Optimisers.AMSGrad"><code>Optimisers.AMSGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AMSGrad(η = 0.001, β = (0.9, 0.999), ϵ = 1e-8)</code></pre><p>The <a href="https://openreview.net/forum?id=ryQu7f-RZ">AMSGrad</a> version of the Adam optimiser. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L423-L436">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.NAdam" href="#Optimisers.NAdam"><code>Optimisers.NAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NAdam(η = 0.001, β = (0.9, 0.999), ϵ = 1e-8)</code></pre><p><a href="https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ">NAdam</a> is a Nesterov variant of Adam. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L458-L471">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AdamW" href="#Optimisers.AdamW"><code>Optimisers.AdamW</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">AdamW(η = 0.001, β = (0.9, 0.999), λ = 0, ϵ = 1e-8)
AdamW(; [eta, beta, lambda, epsilon])</code></pre><p><a href="https://arxiv.org/abs/1711.05101">AdamW</a> is a variant of Adam fixing (as in repairing) its weight decay regularization. Implemented as an <a href="#Optimisers.OptimiserChain"><code>OptimiserChain</code></a> of <a href="#Optimisers.Adam"><code>Adam</code></a> and <a href="#Optimisers.WeightDecay"><code>WeightDecay</code></a>`.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η == eta</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple == beta</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Weight decay (<code>λ == lambda</code>): Controls the strength of <span>$L_2$</span> regularisation.</li><li>Machine epsilon (<code>ϵ == epsilon</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L493-L509">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AdaBelief" href="#Optimisers.AdaBelief"><code>Optimisers.AdaBelief</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaBelief(η = 0.001, β = (0.9, 0.999), ϵ = 1e-16)</code></pre><p>The <a href="https://arxiv.org/abs/2010.07468">AdaBelief</a> optimiser is a variant of the well-known Adam optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ::Float32</code>): Constant to prevent division by zero                                 (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L516-L529">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.Lion" href="#Optimisers.Lion"><code>Optimisers.Lion</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Lion(η = 0.001, β = (0.9, 0.999))</code></pre><p><a href="https://arxiv.org/abs/2302.06675">Lion</a> optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Magnitude by which gradients are updating the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L226-L235">source</a></section></article><p>In addition to the main course, you may wish to order some of these condiments:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AccumGrad" href="#Optimisers.AccumGrad"><code>Optimisers.AccumGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AccumGrad(n::Int)</code></pre><p>A rule constructed <code>OptimiserChain(AccumGrad(n), Rule())</code> will accumulate for <code>n</code> steps, before applying <code>Rule</code> to the mean of these <code>n</code> gradients.</p><p>This is useful for training with effective batch sizes too large for the available memory. Instead of computing the gradient for batch size <code>b</code> at once, compute it for size <code>b/n</code> and accumulate <code>n</code> such gradients.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x=[1f0], y=[2f0]);

julia&gt; r = OptimiserChain(AccumGrad(2), WeightDecay(0.01), Descent(0.1));

julia&gt; s = Optimisers.setup(r, m);

julia&gt; Optimisers.update!(s, m, (x=[33], y=[0]));

julia&gt; m  # model not yet changed
(x = Float32[1.0], y = Float32[2.0])

julia&gt; Optimisers.update!(s, m, (x=[0], y=[444]));

julia&gt; m  # n=2 gradients applied at once
(x = Float32[-0.651], y = Float32[-20.202002])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L747-L775">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.ClipGrad" href="#Optimisers.ClipGrad"><code>Optimisers.ClipGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClipGrad(δ = 10)</code></pre><p>Restricts every gradient component to obey <code>-δ ≤ dx[i] ≤ δ</code>.</p><p>Typically composed with other rules using <a href="#Optimisers.OptimiserChain"><code>OptimiserChain</code></a>.</p><p>See also <a href="#Optimisers.ClipNorm"><code>ClipNorm</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L616-L624">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.ClipNorm" href="#Optimisers.ClipNorm"><code>Optimisers.ClipNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClipNorm(ω = 10, p = 2; throw = true)</code></pre><p>Scales any gradient array for which <code>norm(dx, p) &gt; ω</code> to stay at this threshold (unless <code>p==0</code>).</p><p>Throws an error if the norm is infinite or <code>NaN</code>, which you can turn off with <code>throw = false</code>.</p><p>Typically composed with other rules using <a href="#Optimisers.OptimiserChain"><code>OptimiserChain</code></a>.</p><p>See also <a href="#Optimisers.ClipGrad"><code>ClipGrad</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L638-L650">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.SignDecay" href="#Optimisers.SignDecay"><code>Optimisers.SignDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SignDecay(λ = 1e-3)</code></pre><p>Implements <span>$L_1$</span> regularisation, also known as LASSO regression, when composed  with other rules as the first transformation in an <a href="#Optimisers.OptimiserChain"><code>OptimiserChain</code></a>.</p><p>It does this by adding <code>λ .* sign(x)</code> to the gradient. This is equivalent to adding  <code>λ * sum(abs, x) == λ * norm(x, 1)</code> to the loss.</p><p>See also [<code>WeightDecay</code>] for <span>$L_2$</span> normalisation. They can be used together: <code>OptimiserChain(SignDecay(0.012), WeightDecay(0.034), Adam())</code> is equivalent to adding <code>0.012 * norm(x, 1) + 0.017 * norm(x, 2)^2</code> to the loss function.</p><p><strong>Parameters</strong></p><ul><li>Penalty (<code>λ ≥ 0</code>): Controls the strength of the regularisation.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L586-L601">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.WeightDecay" href="#Optimisers.WeightDecay"><code>Optimisers.WeightDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">WeightDecay(λ = 5e-4)</code></pre><p>Implements <span>$L_2$</span> regularisation, also known as ridge regression,  when composed  with other rules as the first transformation in an <a href="#Optimisers.OptimiserChain"><code>OptimiserChain</code></a>.</p><p>It does this by adding <code>λ .* x</code> to the gradient. This is equivalent to adding  <code>λ/2 * sum(abs2, x) == λ/2 * norm(x)^2</code> to the loss.</p><p>See also [<code>SignDecay</code>] for <span>$L_1$</span> normalisation.</p><p><strong>Parameters</strong></p><ul><li>Penalty (<code>λ ≥ 0</code>): Controls the strength of the regularisation.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L549-L562">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.OptimiserChain" href="#Optimisers.OptimiserChain"><code>Optimisers.OptimiserChain</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OptimiserChain(opts...)</code></pre><p>Compose a sequence of optimisers so that each <code>opt</code> in <code>opts</code> updates the gradient, in the order specified.</p><p>With an empty sequence, <code>OptimiserChain()</code> is the identity, so <code>update!</code> will subtract the full gradient from the parameters. This is equivalent to <code>Descent(1)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; o = OptimiserChain(ClipGrad(1.0), Descent(0.1));

julia&gt; m = (zeros(3),);

julia&gt; s = Optimisers.setup(o, m)
(Leaf(OptimiserChain(ClipGrad(1.0), Descent(0.1)), (nothing, nothing)),)

julia&gt; Optimisers.update(s, m, ([0.3, 1, 7],))[2]  # clips before discounting
([-0.03, -0.1, -0.1],)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/rules.jl#L693-L716">source</a></section></article><h2 id="Model-Interface"><a class="docs-heading-anchor" href="#Model-Interface">Model Interface</a><a id="Model-Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Interface" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.setup" href="#Optimisers.setup"><code>Optimisers.setup</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.setup(rule, model) -&gt; state_tree</code></pre><p>Initialises the given optimiser for every trainable parameter within the model. Returns a tree of the relevant states, which must be passed to <a href="#Optimisers.update"><code>update</code></a> or <a href="#Optimisers.update!"><code>update!</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x = rand(3), y = (true, false), z = tanh);

julia&gt; Optimisers.setup(Momentum(), m)  # same field names as m
(x = Leaf(Momentum(0.01, 0.9), [0.0, 0.0, 0.0]), y = ((), ()), z = ())</code></pre><p>The recursion into structures uses Functors.jl, and any new <code>struct</code>s containing parameters need to be marked with <code>Functors.@functor</code> before use. See <a href="https://fluxml.ai/Flux.jl/stable/models/advanced/">the Flux docs</a> for more about this.</p><pre><code class="language-julia-repl hljs">julia&gt; struct Layer; mat; fun; end

julia&gt; model = (lay = Layer([1 2; 3 4f0], sin), vec = [5, 6f0]);

julia&gt; Optimisers.setup(Momentum(), model)  # new struct is by default ignored
(lay = (), vec = Leaf(Momentum(0.01, 0.9), Float32[0.0, 0.0]))

julia&gt; destructure(model)
(Float32[5.0, 6.0], Restructure(NamedTuple, ..., 2))

julia&gt; using Functors; @functor Layer  # annotate this type as containing parameters

julia&gt; Optimisers.setup(Momentum(), model)
(lay = (mat = Leaf(Momentum(0.01, 0.9), Float32[0.0 0.0; 0.0 0.0]), fun = ()), vec = Leaf(Momentum(0.01, 0.9), Float32[0.0, 0.0]))

julia&gt; destructure(model)
(Float32[1.0, 3.0, 2.0, 4.0, 5.0, 6.0], Restructure(NamedTuple, ..., 6))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/Optimisers.jl#L69-L107">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.update" href="#Optimisers.update"><code>Optimisers.update</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.update(tree, model, gradient) -&gt; (tree, model)</code></pre><p>Uses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from <a href="#Optimisers.setup"><code>setup</code></a>.</p><p>See also <a href="#Optimisers.update!"><code>update!</code></a>, which will be faster for models of ordinary <code>Array</code>s or <code>CuArray</code>s.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x = Float32[1,2,3], y = tanh);

julia&gt; t = Optimisers.setup(Descent(0.1), m)
(x = Leaf(Descent(0.1), nothing), y = ())

julia&gt; g = (x = [1,1,1], y = nothing);  # fake gradient

julia&gt; Optimisers.update(t, m, g)
((x = Leaf(Descent(0.1), nothing), y = ()), (x = Float32[0.9, 1.9, 2.9], y = tanh))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/Optimisers.jl#L110-L131">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.update!" href="#Optimisers.update!"><code>Optimisers.update!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.update!(tree, model, gradient) -&gt; (tree, model)</code></pre><p>Uses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from <a href="#Optimisers.setup"><code>setup</code></a>.</p><p>This is used in exactly the same manner as <a href="#Optimisers.update"><code>update</code></a>, but because it may mutate arrays within the old model (and the old state), it will be faster for models of ordinary <code>Array</code>s or <code>CuArray</code>s. However, you should not rely on the old model being fully updated but rather use the returned model. (The original state tree is always mutated, as each <code>Leaf</code> is mutable.)</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using StaticArrays, Zygote, Optimisers

julia&gt; m = (x = [1f0, 2f0], y = SA[4f0, 5f0]);  # partly mutable model

julia&gt; t = Optimisers.setup(Momentum(1/30, 0.9), m)  # tree of states
(x = Leaf(Momentum(0.0333333, 0.9), Float32[0.0, 0.0]), y = Leaf(Momentum(0.0333333, 0.9), Float32[0.0, 0.0]))

julia&gt; g = gradient(m -&gt; sum(abs2.(m.x .+ m.y)), m)[1]  # structural gradient
(x = Float32[10.0, 14.0], y = Float32[10.0, 14.0])

julia&gt; t2, m2 = Optimisers.update!(t, m, g);

julia&gt; m2  # after update or update!, this is the new model
(x = Float32[0.6666666, 1.5333333], y = Float32[3.6666667, 4.5333333])

julia&gt; m2.x === m.x  # update! has re-used this array, for efficiency
true

julia&gt; m  # original should be discarded, may be mutated but no guarantee
(x = Float32[0.6666666, 1.5333333], y = Float32[4.0, 5.0])

julia&gt; t == t2  # original state tree is guaranteed to be mutated
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/Optimisers.jl#L134-L174">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.adjust!" href="#Optimisers.adjust!"><code>Optimisers.adjust!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.adjust!(tree, η)</code></pre><p>Alters the state <code>tree = setup(rule, model)</code> to change the parameters of the optimisation rule, without destroying its stored state. Typically used mid-way through training.</p><p>Can be applied to part of a model, by acting only on the corresponding part of the state <code>tree</code>.</p><p>To change just the learning rate, provide a number <code>η::Real</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (vec = rand(Float32, 2), fun = sin);

julia&gt; st = Optimisers.setup(Nesterov(), m)  # stored momentum is initialised to zero
(vec = Leaf(Nesterov(0.001, 0.9), Float32[0.0, 0.0]), fun = ())

julia&gt; st, m = Optimisers.update(st, m, (vec = [16, 88], fun = nothing));  # with fake gradient

julia&gt; st
(vec = Leaf(Nesterov(0.001, 0.9), Float32[-0.016, -0.088]), fun = ())

julia&gt; Optimisers.adjust!(st, 0.123)  # change learning rate, stored momentum untouched

julia&gt; st
(vec = Leaf(Nesterov(0.123, 0.9), Float32[-0.016, -0.088]), fun = ())</code></pre><p>To change other parameters, <code>adjust!</code> also accepts keyword arguments matching the field names of the optimisation rule&#39;s type.</p><pre><code class="language-julia-repl hljs">julia&gt; fieldnames(Adam)
(:eta, :beta, :epsilon)

julia&gt; st2 = Optimisers.setup(OptimiserChain(ClipGrad(), Adam()), m)
(vec = Leaf(OptimiserChain(ClipGrad(10.0), Adam(0.001, (0.9, 0.999), 1.0e-8)), (nothing, (Float32[0.0, 0.0], Float32[0.0, 0.0], (0.9, 0.999)))), fun = ())

julia&gt; Optimisers.adjust(st2; beta = (0.777, 0.909), delta = 11.1)  # delta acts on ClipGrad
(vec = Leaf(OptimiserChain(ClipGrad(11.1), Adam(0.001, (0.777, 0.909), 1.0e-8)), (nothing, (Float32[0.0, 0.0], Float32[0.0, 0.0], (0.9, 0.999)))), fun = ())

julia&gt; Optimisers.adjust(st; beta = &quot;no such field&quot;)  # silently ignored!
(vec = Leaf(Nesterov(0.123, 0.9), Float32[-0.016, -0.088]), fun = ())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/adjust.jl#L58-L104">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.adjust-Tuple{Any, Real}" href="#Optimisers.adjust-Tuple{Any, Real}"><code>Optimisers.adjust</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">adjust(tree, η) -&gt; tree</code></pre><p>Like <a href="#Optimisers.adjust-Tuple{Any, Real}"><code>adjust!</code></a>, but returns a new tree instead of mutating the old one.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/adjust.jl#L114-L118">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.freeze!" href="#Optimisers.freeze!"><code>Optimisers.freeze!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.freeze!(tree)</code></pre><p>Temporarily alters the state <code>tree = setup(rule, model)</code> so that parameters will not be updated. Un-done by <a href="#Optimisers.thaw!"><code>thaw!</code></a>.</p><p>Can be applied to the state corresponding to only part of a model, for instance with <code>model::Chain</code>, to freeze <code>model.layers[1]</code> you should call <code>freeze!(tree.layers[1])</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x = ([1.0], 2.0), y = [3.0]);

julia&gt; s = Optimisers.setup(Momentum(), m);

julia&gt; Optimisers.freeze!(s.x)

julia&gt; Optimisers.update!(s, m, (x = ([pi], 10pi), y = [100pi]));  # with fake gradient

julia&gt; m
(x = ([1.0], 2.0), y = [-0.14159265358979312])

julia&gt; s
(x = (Leaf(Momentum(0.01, 0.9), [0.0], frozen = true), ()), y = Leaf(Momentum(0.01, 0.9), [3.14159]))

julia&gt; Optimisers.thaw!(s)

julia&gt; s.x
(Leaf(Momentum(0.01, 0.9), [0.0]), ())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/adjust.jl#L5-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.thaw!" href="#Optimisers.thaw!"><code>Optimisers.thaw!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.thaw!(tree)</code></pre><p>The reverse of <a href="#Optimisers.freeze!"><code>freeze!</code></a>. Applies to all parameters, mutating every <code>Leaf(rule, state, frozen = true)</code> to <code>Leaf(rule, state, frozen = false)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/adjust.jl#L40-L45">source</a></section></article><p>Calling <code>Functors.@functor</code> on your model&#39;s layer types by default causes these functions to recurse into all children, and ultimately optimise all <code>isnumeric</code> leaf nodes. To further restrict this by ignoring some fields of a layer type, define <code>trainable</code>:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.trainable" href="#Optimisers.trainable"><code>Optimisers.trainable</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trainable(x::Layer) -&gt; NamedTuple</code></pre><p>This may be overloaded to make optimisers ignore some fields of every <code>Layer</code>, which would otherwise contain trainable parameters.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This is very rarely required. Fields of <code>struct Layer</code> which contain functions, or integers like sizes, are always ignored anyway. Overloading <code>trainable</code> is only necessary when some arrays of numbers are to be optimised, and some arrays of numbers are not.</p></div></div><p>The default is <code>Functors.children(x)</code>, usually a NamedTuple of all fields, and <code>trainable(x)</code> must contain a subset of these.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/interface.jl#L153-L167">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.isnumeric" href="#Optimisers.isnumeric"><code>Optimisers.isnumeric</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">isnumeric(x) -&gt; Bool</code></pre><p>Returns <code>true</code> on any parameter to be adjusted by Optimisers.jl, namely arrays of non-integer numbers. Returns <code>false</code> on all other types.</p><p>Requires also that <code>Functors.isleaf(x) == true</code>, to focus on e.g. the parent of a transposed matrix, not the wrapper.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/interface.jl#L128-L136">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.maywrite" href="#Optimisers.maywrite"><code>Optimisers.maywrite</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">maywrite(x) -&gt; Bool</code></pre><p>Should return <code>true</code> if we are completely sure that <code>update!</code> can write new values into <code>x</code>. Otherwise <code>false</code>, indicating a non-mutating path. For now, simply <code>x isa DenseArray</code> allowing <code>Array</code>, <code>CuArray</code>, etc. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/interface.jl#L141-L147">source</a></section></article><p>Such restrictions are also obeyed by this function for flattening a model:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.destructure" href="#Optimisers.destructure"><code>Optimisers.destructure</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">destructure(model) -&gt; vector, reconstructor</code></pre><p>Copies all <a href="#Optimisers.trainable"><code>trainable</code></a>, <a href="#Optimisers.isnumeric"><code>isnumeric</code></a> parameters in the model to a vector, and returns also a function which reverses this transformation. Differentiable.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; v, re = destructure((x=[1.0, 2.0], y=(sin, [3.0 + 4.0im])))
(ComplexF64[1.0 + 0.0im, 2.0 + 0.0im, 3.0 + 4.0im], Restructure(NamedTuple, ..., 3))

julia&gt; re([3, 5, 7+11im])
(x = [3.0, 5.0], y = (sin, ComplexF64[7.0 + 11.0im]))</code></pre><p>If <code>model</code> contains various number types, they are promoted to make <code>vector</code>, and are usually restored by <code>Restructure</code>. Such restoration follows the rules  of <code>ChainRulesCore.ProjectTo</code>, and thus will restore floating point precision, but will permit more exotic numbers like <code>ForwardDiff.Dual</code>.</p><p>If <code>model</code> contains only GPU arrays, then <code>vector</code> will also live on the GPU. At present, a mixture of GPU and ordinary CPU arrays is undefined behaviour.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/destructure.jl#L5-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.Restructure" href="#Optimisers.Restructure"><code>Optimisers.Restructure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Restructure(Model, ..., length)</code></pre><p>This is what <a href="#Optimisers.destructure"><code>destructure</code></a> returns, and <code>re(p)</code> will re-build the model with new parameters from vector <code>p</code>. If the model is callable, then <code>re(x, p) == re(p)(x)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; using Flux, Optimisers

julia&gt; _, re = destructure(Dense([1 2; 3 4], [0, 0], sigmoid))
([1, 3, 2, 4, 0, 0], Restructure(Dense, ..., 6))

julia&gt; m = re(-4:1)
Dense(2, 2, σ)      # 6 parameters

julia&gt; m([0.2, 0.3]) ≈ re([0.2, 0.3], -4:1)
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/destructure.jl#L34-L53">source</a></section></article><h2 id="Rule-Definition"><a class="docs-heading-anchor" href="#Rule-Definition">Rule Definition</a><a id="Rule-Definition-1"></a><a class="docs-heading-anchor-permalink" href="#Rule-Definition" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.apply!" href="#Optimisers.apply!"><code>Optimisers.apply!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.apply!(rule::RuleType, state, parameters, gradient) -&gt; (state, gradient)</code></pre><p>This defines the action of any optimisation rule. It should return the modified gradient which will be subtracted from the parameters, and the updated state (if any) for use at the next iteration, as a tuple <code>(state, gradient)</code>.</p><p>For efficiency it is free to mutate the old state, but only what is returned will be used. Ideally this should check <code>maywrite(x)</code>, which the built-in rules do via <a href="#Optimisers.@.."><code>@..</code></a>.</p><p>The initial state is <code>init(rule::RuleType, parameters)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Optimisers.init(Descent(0.1), Float32[1,2,3]) === nothing
true

julia&gt; Optimisers.apply!(Descent(0.1), nothing, Float32[1,2,3], [4,5,6])
(nothing, Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}}(*, ([4, 5, 6], 0.1f0)))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/Optimisers.jl#L24-L44">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.init" href="#Optimisers.init"><code>Optimisers.init</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.init(rule::RuleType, parameters) -&gt; state</code></pre><p>Sets up the initial state for a given optimisation rule, and an array of parameters. This and <a href="#Optimisers.apply!"><code>apply!</code></a> are the two functions which any new optimisation rule must define.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Optimisers.init(Descent(), Float32[1,2,3])  # is `nothing`

julia&gt; Optimisers.init(Momentum(), [1.0, 2.0])
2-element Vector{Float64}:
 0.0
 0.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/Optimisers.jl#L47-L62">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.@.." href="#Optimisers.@.."><code>Optimisers.@..</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">@.. x = y + z</code></pre><p>Sometimes in-place broadcasting macro, for use in <code>apply!</code> rules. If <code>maywrite(x)</code> then it is just <code>@. x = rhs</code>, but if not, it becomes <code>x = @. rhs</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/interface.jl#L194-L199">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.@lazy" href="#Optimisers.@lazy"><code>Optimisers.@lazy</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">x = @lazy y + z</code></pre><p>Lazy broadcasting macro, for use in <code>apply!</code> rules. It broadcasts like <code>@.</code> but does not materialise, returning a <code>Broadcasted</code> object for later use. Beware that mutation of arguments will affect the result, and that if it is used in two places, work will be done twice.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/interface.jl#L211-L218">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.adjust-Tuple{AbstractRule, Real}" href="#Optimisers.adjust-Tuple{AbstractRule, Real}"><code>Optimisers.adjust</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">Optimisers.adjust(rule::RuleType, η::Real) -&gt; rule</code></pre><p>If a new optimisation rule has a learning rate which is <em>not</em> stored in field <code>rule.eta</code>, then you may should add a method to <code>adjust</code>. (But simpler to just use the standard name.)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/adjust.jl#L130-L135">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.@def" href="#Optimisers.@def"><code>Optimisers.@def</code></a> — <span class="docstring-category">Macro</span></header><section><div><p>@def struct Rule; eta = 0.1; beta = (0.7, 0.8); end</p><p>Helper macro for defining rules with default values. The types of the literal values are used in the <code>struct</code>, like this:</p><pre><code class="nohighlight hljs">struct Rule
  eta::Float64
  beta::Tuple{Float64, Float64}
  Rule(eta, beta = (0.7, 0.8)) = eta &lt; 0 ? error() : new(eta, beta)
  Rule(; eta = 0.1, beta = (0.7, 0.8)) = Rule(eta, beta)
end</code></pre><p>Any field called <code>eta</code> is assumed to be a learning rate, and cannot be negative.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/07b8ee7130be2b01fd99ddebef60d2639dcc0171/src/interface.jl#L234-L249">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Tuesday 2 April 2024 12:34">Tuesday 2 April 2024</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
